{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74c90134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@'         # define a string of punctuation symbols\n",
    "\n",
    "# Functions to clean tweets\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
    "    return tweet\n",
    "\n",
    "def remove_av(tweet):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
    "    return tweet\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Returns tokenized representation of words in lemma form excluding stopwords\"\"\"\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(tweet):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS \\\n",
    "                and len(token) > 2:  # drops words with less than 3 characters\n",
    "            result.append(lemmatize(token))\n",
    "    return result\n",
    "\n",
    "def lemmatize(token):\n",
    "    \"\"\"Returns lemmatization of a token\"\"\"\n",
    "    return WordNetLemmatizer().lemmatize(token, pos='v')\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"Main master function to clean tweets, stripping noisy characters, and tokenizing use lemmatization\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    #tweet = tweet.lower()  # lower case\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    #tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    #tweet_token_list = tokenize(tweet)  # apply lemmatization and tokenization\n",
    "    #tweet = ' '.join(tweet_token_list)\n",
    "    return tweet\n",
    "\n",
    "def basic_clean(tweet):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet = re.sub('üìù ‚Ä¶', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def tokenize_tweets(df):\n",
    "    \"\"\"Main function to read in and return cleaned and preprocessed dataframe.\n",
    "    This can be used in Jupyter notebooks by importing this module and calling the tokenize_tweets() function\n",
    "    Args:\n",
    "        df = data frame object to apply cleaning to\n",
    "    Returns:\n",
    "        pandas data frame with cleaned tokens\n",
    "    \"\"\"\n",
    "\n",
    "    df['tokens'] = df.speech.apply(preprocess_tweet)\n",
    "    num_tweets = len(df)\n",
    "    print('Complete. Number of Tweets that have been cleaned and tokenized : {}'.format(num_tweets))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fc44ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = pd.read_csv(\"new_paper_hatedata.csv\")\n",
    "newdata = newdata.drop(columns = [\"Index\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99e2b981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of Tweets that have been cleaned and tokenized : 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0,Hopefully they investigate the diseases that...</td>\n",
       "      <td>0 Hopefully they investigate the diseases that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1,Horrible 'Nigerian disease' called Monkeypox...</td>\n",
       "      <td>1 Horrible Nigerian disease called Monkeypox s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2,@user You forgot the mass uncontrolled 3rd w...</td>\n",
       "      <td>2 You forgot the mass uncontrolled 3rd world i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3,We should close the borders until the Wall i...</td>\n",
       "      <td>3 We should close the borders until the Wall i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4,While they bring disease and ruin your walls...</td>\n",
       "      <td>4 While they bring disease and ruin your walls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95,RT @user : The next question is why the gov...</td>\n",
       "      <td>95 The next question is why the government has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96,I have taught BOTH my daughters and son tha...</td>\n",
       "      <td>96 I have taught BOTH my daughters and son tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97,Call them what they are ILLEGAL ALIENS not ...</td>\n",
       "      <td>97 Call them what they are ILLEGAL ALIENS not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98,@user We've heard 11 million illegal aliens...</td>\n",
       "      <td>98 We ve heard 11 million illegal aliens for d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99,@user LIVE BY OUR LAWS OR GO BACK FROM WHEN...</td>\n",
       "      <td>99 LIVE BY OUR LAWS OR GO BACK FROM WHENCE YOU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               speech  \\\n",
       "0   0,Hopefully they investigate the diseases that...   \n",
       "1   1,Horrible 'Nigerian disease' called Monkeypox...   \n",
       "2   2,@user You forgot the mass uncontrolled 3rd w...   \n",
       "3   3,We should close the borders until the Wall i...   \n",
       "4   4,While they bring disease and ruin your walls...   \n",
       "..                                                ...   \n",
       "95  95,RT @user : The next question is why the gov...   \n",
       "96  96,I have taught BOTH my daughters and son tha...   \n",
       "97  97,Call them what they are ILLEGAL ALIENS not ...   \n",
       "98  98,@user We've heard 11 million illegal aliens...   \n",
       "99  99,@user LIVE BY OUR LAWS OR GO BACK FROM WHEN...   \n",
       "\n",
       "                                               tokens  \n",
       "0   0 Hopefully they investigate the diseases that...  \n",
       "1   1 Horrible Nigerian disease called Monkeypox s...  \n",
       "2   2 You forgot the mass uncontrolled 3rd world i...  \n",
       "3   3 We should close the borders until the Wall i...  \n",
       "4   4 While they bring disease and ruin your walls...  \n",
       "..                                                ...  \n",
       "95  95 The next question is why the government has...  \n",
       "96  96 I have taught BOTH my daughters and son tha...  \n",
       "97  97 Call them what they are ILLEGAL ALIENS not ...  \n",
       "98  98 We ve heard 11 million illegal aliens for d...  \n",
       "99  99 LIVE BY OUR LAWS OR GO BACK FROM WHENCE YOU...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_newdata = tokenize_tweets(newdata)\n",
    "clean_newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18393611",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_newdata.to_csv(\"paper_data_annotation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e326c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
