{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lei-yang5/COMP8240project/blob/main/newData1Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Hqb1OvsXDR",
        "outputId": "552d14d8-f799-4e0c-80f9-f762f76d873d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'HateXplain' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/punyajoy/HateXplain.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FPAJpqcYVEM0"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l4M1_SHtDIm",
        "outputId": "955801d5-2515-4e5d-e7be-edd1a794329d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HateXplain\n"
          ]
        }
      ],
      "source": [
        "cd HateXplain/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vl3PF9alWjHx"
      },
      "outputs": [],
      "source": [
        "!mkdir Saved/\n",
        "!mkdir explanations_dicts/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU8P_dV5Wnk5",
        "outputId": "b1429f33-5a3c-42eb-a9e0-619379ffa871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-30 05:41:05--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2022-10-30 05:41:05--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
            "--2022-10-30 05:41:06--  https://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘Data/glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  5.13MB/s    in 5m 56s  \n",
            "\n",
            "2022-10-30 05:47:03 (5.03 MB/s) - ‘Data/glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip  -P Data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZi9IlH-W_83",
        "outputId": "be2ee47f-a47c-4dd7-86fe-0f6482183277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Data/glove.42B.300d.zip\n",
            "  inflating: Data/glove.42B.300d.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip Data/glove.42B.300d.zip -d Data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gofRAO-crNgI"
      },
      "outputs": [],
      "source": [
        "!rm Data/glove.42B.300d.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h0akw80ipBIu",
        "outputId": "b920ff12-c0f2-4aa3-ede5-d77012f73c8c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scipy==1.4.1\n",
            "  Using cached scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
            "Collecting tqdm==4.43.0\n",
            "  Using cached tqdm-4.43.0-py2.py3-none-any.whl (59 kB)\n",
            "Collecting Keras==2.3.1\n",
            "  Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
            "Collecting waiting==1.4.1\n",
            "  Using cached waiting-1.4.1.tar.gz (7.1 kB)\n",
            "Collecting ekphrasis==0.5.1\n",
            "  Using cached ekphrasis-0.5.1.tar.gz (80 kB)\n",
            "Collecting pandas==1.0.3\n",
            "  Using cached pandas-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (10.0 MB)\n",
            "Collecting transformers==2.5.1\n",
            "  Using cached transformers-2.5.1-py3-none-any.whl (499 kB)\n",
            "Collecting lime==0.2.0.1\n",
            "  Using cached lime-0.2.0.1.tar.gz (275 kB)\n",
            "Collecting numpy==1.16.3\n",
            "  Using cached numpy-1.16.3-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n",
            "Collecting matplotlib==3.2.1\n",
            "  Using cached matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting neptune_client==0.4.107\n",
            "  Using cached neptune-client-0.4.107.tar.gz (86 kB)\n",
            "Collecting knockknock==0.1.7\n",
            "  Using cached knockknock-0.1.7-py3-none-any.whl (19 kB)\n",
            "Collecting torch==1.1.0\n",
            "  Using cached torch-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (676.9 MB)\n",
            "Collecting apex==0.9.10dev\n",
            "  Using cached apex-0.9.10dev.tar.gz (36 kB)\n",
            "Collecting dataclasses==0.6\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting GPUtil==1.4.0\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Collecting scikit_learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (6.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (3.1.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (1.1.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (2.0.1)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (5.5.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (3.7)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.3->-r requirements.txt (line 7)) (2022.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.3->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (3.8.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "  Downloading tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 22.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 68.7 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.25.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 70.0 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 37.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime==0.2.0.1->-r requirements.txt (line 9)) (0.18.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 11)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 11)) (1.4.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 12)) (5.2.1)\n",
            "Collecting bravado\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting py3nvml\n",
            "  Downloading py3nvml-0.2.7-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (3.2.2)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (7.1.2)\n",
            "Collecting PyJWT\n",
            "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (1.3.1)\n",
            "Collecting websocket-client>=0.35.0\n",
            "  Downloading websocket_client-1.4.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 71.4 MB/s \n",
            "\u001b[?25hCollecting yagmail>=0.11.214\n",
            "  Downloading yagmail-0.15.293-py2.py3-none-any.whl (17 kB)\n",
            "Collecting twilio\n",
            "  Downloading twilio-7.15.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 58.0 MB/s \n",
            "\u001b[?25hCollecting keyring\n",
            "  Downloading keyring-23.9.3-py3-none-any.whl (35 kB)\n",
            "Collecting python-telegram-bot\n",
            "  Downloading python_telegram_bot-13.14-py3-none-any.whl (514 kB)\n",
            "\u001b[K     |████████████████████████████████| 514 kB 71.3 MB/s \n",
            "\u001b[?25hCollecting matrix-client\n",
            "  Downloading matrix_client-0.4.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting cryptacular\n",
            "  Downloading cryptacular-1.6.2.tar.gz (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zope.sqlalchemy\n",
            "  Downloading zope.sqlalchemy-1.6-py2.py3-none-any.whl (22 kB)\n",
            "Collecting velruse>=1.0.3\n",
            "  Downloading velruse-1.1.1.tar.gz (709 kB)\n",
            "\u001b[K     |████████████████████████████████| 709 kB 66.0 MB/s \n",
            "\u001b[?25hCollecting pyramid>1.1.2\n",
            "  Downloading pyramid-2.0-py3-none-any.whl (246 kB)\n",
            "\u001b[K     |████████████████████████████████| 246 kB 69.7 MB/s \n",
            "\u001b[?25hCollecting pyramid_mailer\n",
            "  Downloading pyramid_mailer-0.15.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting wtforms\n",
            "  Downloading WTForms-3.0.1-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 64.7 MB/s \n",
            "\u001b[?25hCollecting wtforms-recaptcha\n",
            "  Downloading wtforms_recaptcha-0.3.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.23.2->-r requirements.txt (line 19)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.23.2->-r requirements.txt (line 19)) (3.1.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune_client==0.4.107->-r requirements.txt (line 13)) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting plaster-pastedeploy\n",
            "  Downloading plaster_pastedeploy-0.7-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting hupper>=1.5\n",
            "  Downloading hupper-1.10.3-py2.py3-none-any.whl (26 kB)\n",
            "Collecting webob>=1.8.3\n",
            "  Downloading WebOb-1.8.7-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 71.0 MB/s \n",
            "\u001b[?25hCollecting zope.deprecation>=3.5.0\n",
            "  Downloading zope.deprecation-4.4.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting zope.interface>=3.8.0\n",
            "  Downloading zope.interface-5.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (254 kB)\n",
            "\u001b[K     |████████████████████████████████| 254 kB 53.4 MB/s \n",
            "\u001b[?25hCollecting plaster\n",
            "  Downloading plaster-1.0-py2.py3-none-any.whl (14 kB)\n",
            "Collecting translationstring>=0.4\n",
            "  Downloading translationstring-1.4-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (57.4.0)\n",
            "Collecting venusian>=1.0\n",
            "  Downloading venusian-3.0.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 8)) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9)) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9)) (2021.11.2)\n",
            "Collecting scikit-image>=0.12\n",
            "  Downloading scikit_image-0.19.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 45.8 MB/s \n",
            "\u001b[?25h  Downloading scikit_image-0.19.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 25.7 MB/s \n",
            "\u001b[?25h  Downloading scikit_image-0.19.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.3 MB 49.3 MB/s \n",
            "\u001b[?25h  Downloading scikit_image-0.19.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (55.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 55.4 MB 21 kB/s \n",
            "\u001b[?25h  Downloading scikit_image-0.18.2-cp37-cp37m-manylinux1_x86_64.whl (29.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.2 MB 83.8 MB/s \n",
            "\u001b[?25h  Downloading scikit_image-0.18.1-cp37-cp37m-manylinux1_x86_64.whl (29.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.2 MB 2.8 MB/s \n",
            "\u001b[?25h  Downloading scikit_image-0.18.0-cp37-cp37m-manylinux1_x86_64.whl (29.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.2 MB 89 kB/s \n",
            "\u001b[?25h  Downloading scikit_image-0.17.2-cp37-cp37m-manylinux1_x86_64.whl (12.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.5 MB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9)) (2.6.3)\n",
            "Collecting PyWavelets>=1.1.1\n",
            "  Downloading PyWavelets-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (6.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.1 MB 34.9 MB/s \n",
            "\u001b[?25h  Downloading PyWavelets-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 57.5 MB/s \n",
            "\u001b[?25hCollecting anykeystore\n",
            "  Downloading anykeystore-0.2.tar.gz (10 kB)\n",
            "Collecting python3-openid\n",
            "  Downloading python3_openid-3.2.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 64.8 MB/s \n",
            "\u001b[?25hCollecting premailer\n",
            "  Downloading premailer-3.10.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting botocore<1.29.0,>=1.28.4\n",
            "  Downloading botocore-1.28.4-py3-none-any.whl (9.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.3 MB 54.6 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 72.1 MB/s \n",
            "\u001b[?25hCollecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.1-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.0.4)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 69.3 MB/s \n",
            "\u001b[?25hCollecting jsonref\n",
            "  Downloading jsonref-1.0.0.post1-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (4.3.3)\n",
            "Collecting swagger-spec-validator>=2.0.1\n",
            "  Downloading swagger_spec_validator-3.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (22.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (4.13.0)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting rfc3339-validator\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting uri-template\n",
            "  Downloading uri_template-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting isoduration\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting fqdn\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting jsonpointer>1.13\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting webcolors>=1.11\n",
            "  Downloading webcolors-1.12-py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (3.9.0)\n",
            "Collecting pbkdf2\n",
            "  Downloading pbkdf2-1.3.tar.gz (6.4 kB)\n",
            "Requirement already satisfied: cached-property>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from fqdn->jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.5.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis==0.5.1->-r requirements.txt (line 6)) (0.2.5)\n",
            "Collecting arrow>=0.15.0\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting jeepney>=0.4.2\n",
            "  Downloading jeepney-0.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.3-py3-none-any.whl (6.0 kB)\n",
            "Collecting SecretStorage>=3.2\n",
            "  Downloading SecretStorage-3.3.3-py3-none-any.whl (15 kB)\n",
            "Collecting cryptography>=2.0\n",
            "  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (2.21)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from jaraco.classes->keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (9.0.0)\n",
            "Collecting PasteDeploy>=2.0\n",
            "  Downloading PasteDeploy-3.0.1-py3-none-any.whl (16 kB)\n",
            "Collecting cssselect\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting cssutils\n",
            "  Downloading cssutils-2.6.0-py3-none-any.whl (399 kB)\n",
            "\u001b[K     |████████████████████████████████| 399 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14)) (4.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14)) (4.9.1)\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting repoze.sendmail>=4.1\n",
            "  Downloading repoze.sendmail-4.4.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 36 kB/s \n",
            "\u001b[?25hCollecting transaction\n",
            "  Downloading transaction-3.0.1-py2.py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting tornado==6.1\n",
            "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
            "\u001b[K     |████████████████████████████████| 428 kB 68.4 MB/s \n",
            "\u001b[?25hCollecting APScheduler==3.6.3\n",
            "  Downloading APScheduler-3.6.3-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting cachetools\n",
            "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14)) (1.5.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from python3-openid->velruse>=1.0.3->apex==0.9.10dev->-r requirements.txt (line 16)) (0.7.1)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.7/dist-packages (from wtforms->apex==0.9.10dev->-r requirements.txt (line 16)) (2.0.1)\n",
            "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9 in /usr/local/lib/python3.7/dist-packages (from zope.sqlalchemy->apex==0.9.10dev->-r requirements.txt (line 16)) (1.4.42)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9->zope.sqlalchemy->apex==0.9.10dev->-r requirements.txt (line 16)) (1.1.3.post0)\n",
            "Building wheels for collected packages: waiting, ekphrasis, lime, neptune-client, apex, GPUtil, future, velruse, anykeystore, cryptacular, pbkdf2, sacremoses\n",
            "  Building wheel for waiting (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for waiting: filename=waiting-1.4.1-py3-none-any.whl size=3761 sha256=e96dda8fc27b2264299a53f5232112ec1fdc7b48b4d46c4b9d8e9449abc77c0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/e5/bb/ee3da3e19f6bffa54131b5039cb25b5eca9e4981a248504840\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-py3-none-any.whl size=82842 sha256=0915fed2883b03bd2338399f0ef57e79313aa4b215fbf3ceacc07e404d37db61\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/ec/0d/12659e32faf780546945d0120f2c8410eb3efb7426731da88f\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283857 sha256=7b5a865ff4612fbfc8472cbac5489e7021bf8dc9ea53c96fb4872523c2660cdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/cb/e5/ac701e12d365a08917bf4c6171c0961bc880a8181359c66aa7\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.4.107-py2.py3-none-any.whl size=145062 sha256=59ffd779aba57e4e07ab9da68e64a971d18af2c8b375466432ae929ab55e74d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/2b/f8/91012c0b3f7a4fe6a3907b274b69cac53bb9edacef63909f01\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.9.10.dev0-py3-none-any.whl size=46467 sha256=5a800c75e9eb65b4fd626dc6ac5592ecc1de093fcd637c3d5ee2d57389d0aa02\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/00/2b/37b6028388b451bbd30230c62f5238aef3b11fdff9503138bc\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=a6d0af2a0750353ca6112de0f04dcfd3197e1a93b8941c8250d83a5cec8a006f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=69f7bccf84347d14773e79ad0d28456fedcda5aa5ee367b7fd4d21511b757872\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for velruse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for velruse: filename=velruse-1.1.1-py3-none-any.whl size=50938 sha256=38b36505791060128412e2f3fb21197438b517026af4a18db59199b77b89818d\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f0/95/7f8b3bb1cce5c78ca7a7922cf72383f886f70e358f0a18d60b\n",
            "  Building wheel for anykeystore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for anykeystore: filename=anykeystore-0.2-py3-none-any.whl size=17043 sha256=fa3260c42e314c84a38f9e79482aaaebb794fbf62452459e84ec8ee9210476ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/14/12/afad2dc2b7ea0884e12f260b0723b49b98f39f08adb7b0414f\n",
            "  Building wheel for cryptacular (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cryptacular: filename=cryptacular-1.6.2-cp37-cp37m-linux_x86_64.whl size=54546 sha256=c172efbd9b7792d014f7e014cf3714364788734a75d9e44b1649752999adc167\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/e8/c2/4b71f45f434136d31df930960b8a916bb36ebe2144479a37a1\n",
            "  Building wheel for pbkdf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pbkdf2: filename=pbkdf2-1.3-py3-none-any.whl size=5105 sha256=aa78ea6ad5d82acecf5f3e4b6b36bd7586c78620cdca4eeae06f4295b01b20b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/16/ea/daca297d70ee0782ac6e16e83b2c55b2ca42a2113750bc0489\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=d75aa8eae31753e36286c37ef9b9cc314da4496608280d9221ebe2289fe69032\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built waiting ekphrasis lime neptune-client apex GPUtil future velruse anykeystore cryptacular pbkdf2 sacremoses\n",
            "Installing collected packages: arrow, zope.interface, webcolors, urllib3, uri-template, rfc3987, rfc3339-validator, plaster, PasteDeploy, jsonpointer, jmespath, isoduration, fqdn, zope.deprecation, webob, venusian, translationstring, transaction, swagger-spec-validator, smmap, simplejson, plaster-pastedeploy, numpy, jsonref, jeepney, hupper, cssutils, cssselect, cryptography, cachetools, botocore, xmltodict, wtforms, tqdm, tornado, SecretStorage, scipy, s3transfer, repoze.sendmail, PyWavelets, python3-openid, pyramid, PyJWT, premailer, pbkdf2, monotonic, matplotlib, jaraco.classes, gitdb, bravado-core, APScheduler, anykeystore, zope.sqlalchemy, yagmail, wtforms-recaptcha, websocket-client, velruse, twilio, tokenizers, sentencepiece, scikit-learn, scikit-image, sacremoses, python-telegram-bot, pyramid-mailer, py3nvml, pandas, matrix-client, keyring, keras-applications, GitPython, future, ftfy, cryptacular, colorama, bravado, boto3, waiting, transformers, torch, neptune-client, lime, knockknock, Keras, GPUtil, gensim, ekphrasis, dataclasses, apex\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 4.2.4\n",
            "    Uninstalling cachetools-4.2.4:\n",
            "      Successfully uninstalled cachetools-4.2.4\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: PyWavelets\n",
            "    Found existing installation: PyWavelets 1.3.0\n",
            "    Uninstalling PyWavelets-1.3.0:\n",
            "      Successfully uninstalled PyWavelets-1.3.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.1\n",
            "    Uninstalling tokenizers-0.13.1:\n",
            "      Successfully uninstalled tokenizers-0.13.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.18.3\n",
            "    Uninstalling scikit-image-0.18.3:\n",
            "      Successfully uninstalled scikit-image-0.18.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.23.1\n",
            "    Uninstalling transformers-4.23.1:\n",
            "      Successfully uninstalled transformers-4.23.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: Keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.16.3 which is incompatible.\n",
            "xarray 0.20.2 requires pandas>=1.1, but you have pandas 1.0.3 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.16.3 which is incompatible.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.1.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.1.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.1.0 which is incompatible.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires keras<2.9,>=2.8.0rc0, but you have keras 2.3.1 which is incompatible.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires numpy>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.16.3 which is incompatible.\n",
            "resampy 0.4.2 requires numpy>=1.17, but you have numpy 1.16.3 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.16.3 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.16.3 which is incompatible.\n",
            "prophet 1.1.1 requires pandas>=1.0.4, but you have pandas 1.0.3 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.16.3 which is incompatible.\n",
            "plotnine 0.8.0 requires pandas>=1.1.0, but you have pandas 1.0.3 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.43.0 which is incompatible.\n",
            "numba 0.56.3 requires numpy<1.24,>=1.18, but you have numpy 1.16.3 which is incompatible.\n",
            "mizani 0.7.3 requires pandas>=1.1.0, but you have pandas 1.0.3 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.3 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.23 requires numpy>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "jax 0.3.23 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "gym 0.25.2 requires numpy>=1.18.0, but you have numpy 1.16.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0, but you have pandas 1.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.1 which is incompatible.\n",
            "fastai 2.7.9 requires torch<1.14,>=1.7, but you have torch 1.1.0 which is incompatible.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "cmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.16.3 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.16.3 which is incompatible.\n",
            "aesara 2.7.9 requires numpy>=1.17.0, but you have numpy 1.16.3 which is incompatible.\n",
            "aeppl 0.0.33 requires numpy>=1.18.1, but you have numpy 1.16.3 which is incompatible.\u001b[0m\n",
            "Successfully installed APScheduler-3.6.3 GPUtil-1.4.0 GitPython-3.1.29 Keras-2.3.1 PasteDeploy-3.0.1 PyJWT-2.6.0 PyWavelets-1.1.1 SecretStorage-3.3.3 anykeystore-0.2 apex-0.9.10.dev0 arrow-1.2.3 boto3-1.25.4 botocore-1.28.4 bravado-11.0.3 bravado-core-5.17.1 cachetools-4.2.2 colorama-0.4.6 cryptacular-1.6.2 cryptography-38.0.1 cssselect-1.2.0 cssutils-2.6.0 dataclasses-0.6 ekphrasis-0.5.1 fqdn-1.5.1 ftfy-6.1.1 future-0.18.2 gensim-3.8.1 gitdb-4.0.9 hupper-1.10.3 isoduration-20.11.0 jaraco.classes-3.2.3 jeepney-0.8.0 jmespath-1.0.1 jsonpointer-2.3 jsonref-1.0.0.post1 keras-applications-1.0.8 keyring-23.9.3 knockknock-0.1.7 lime-0.2.0.1 matplotlib-3.2.1 matrix-client-0.4.0 monotonic-1.6 neptune-client-0.4.107 numpy-1.16.3 pandas-1.0.3 pbkdf2-1.3 plaster-1.0 plaster-pastedeploy-0.7 premailer-3.10.0 py3nvml-0.2.7 pyramid-2.0 pyramid-mailer-0.15.1 python-telegram-bot-13.14 python3-openid-3.2.0 repoze.sendmail-4.4.1 rfc3339-validator-0.1.4 rfc3987-1.3.8 s3transfer-0.6.0 sacremoses-0.0.53 scikit-image-0.17.2 scikit-learn-0.23.2 scipy-1.4.1 sentencepiece-0.1.97 simplejson-3.17.6 smmap-5.0.0 swagger-spec-validator-3.0.2 tokenizers-0.5.2 torch-1.1.0 tornado-6.1 tqdm-4.43.0 transaction-3.0.1 transformers-2.5.1 translationstring-1.4 twilio-7.15.0 uri-template-1.2.0 urllib3-1.25.11 velruse-1.1.1 venusian-3.0.0 waiting-1.4.1 webcolors-1.12 webob-1.8.7 websocket-client-1.4.1 wtforms-3.0.1 wtforms-recaptcha-0.3.2 xmltodict-0.13.0 yagmail-0.15.293 zope.deprecation-4.4.0 zope.interface-5.5.0 zope.sqlalchemy-1.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nlq-y7f7nxdP"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec('Data/glove.42B.300d.txt', 'Data/glove.42B.300d_w2v.txt')\n",
        "word2vecmodel1 = KeyedVectors.load_word2vec_format('Data/glove.42B.300d_w2v.txt', binary=False)\n",
        "word2vecmodel1.save(\"Data/word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrVyCVHdlWSc",
        "outputId": "44ca3e43-a1d0-4eb8-b5a6-3d33464fe054"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import gc\n",
        "del word2vecmodel1\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfpYO5kshKMf",
        "outputId": "54bdc6f2-7f57-4aff-cb58-40ec3382e187"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.43.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.16.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install keras==2.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PrJCxwIhr3o",
        "outputId": "7c13383a-da3d-499d-fbb1-7e6f6154439b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.8\n",
            "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires numpy>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.3 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1E2N1U2plHBA",
        "outputId": "dd8d64cd-ba28-428b-a5fa-aeb12f594e36"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.8 in /usr/local/lib/python3.7/dist-packages (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (4.1.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.2.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.50.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.3.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.17.3)\n",
            "Collecting numpy>=1.20\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 32.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.0.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (14.0.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.27.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.8) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.16.3\n",
            "    Uninstalling numpy-1.16.3:\n",
            "      Successfully uninstalled numpy-1.16.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "xarray 0.20.2 requires pandas>=1.1, but you have pandas 1.0.3 which is incompatible.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.1.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.1.0 which is incompatible.\n",
            "prophet 1.1.1 requires pandas>=1.0.4, but you have pandas 1.0.3 which is incompatible.\n",
            "plotnine 0.8.0 requires pandas>=1.1.0, but you have pandas 1.0.3 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "mizani 0.7.3 requires pandas>=1.1.0, but you have pandas 1.0.3 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.23 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0, but you have pandas 1.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.1 which is incompatible.\n",
            "fastai 2.7.9 requires torch<1.14,>=1.7, but you have torch 1.1.0 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5HS3NNHzmX64"
      },
      "outputs": [],
      "source": [
        "!rm Data/glove.42B.300d.txt\n",
        "!rm Data/glove.42B.300d_w2v.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53QcDc7AqkK5",
        "outputId": "22dd1991-9c84-4234-dc49-2a85ddc3a1b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n",
            "Reading english - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/english/counts_1grams.txt\n"
          ]
        }
      ],
      "source": [
        "from manual_training_inference import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIKH2h5hzwcT",
        "outputId": "b2b4c1c6-83a9-480d-9ef1-43f14bb23afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Since you dont want to use GPU, using the CPU instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 60/20148 [00:00<00:33, 594.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_data 20148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20148/20148 [00:39<00:00, 515.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_error: 0\n",
            "no_majority: 919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 1386/15383 [00:00<00:02, 6898.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15383/15383 [00:02<00:00, 6635.42it/s]\n",
            "  5%|▌         | 803/15383 [00:00<00:01, 8022.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(22236, 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15383/15383 [00:02<00:00, 5764.10it/s]\n",
            "100%|██████████| 1922/1922 [00:00<00:00, 7879.93it/s]\n",
            "100%|██████████| 1924/1924 [00:00<00:00, 7621.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total dataset size: 19229\n",
            "[1.2301791 0.8423818]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [03:04,  2.61it/s]\n",
            "2it [00:00, 12.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.3130810394604\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:37, 12.87it/s]\n",
            "2it [00:00, 13.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.67\n",
            " Fscore: 0.66\n",
            " Precision: 0.74\n",
            " Recall: 0.71\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:38\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:04, 12.82it/s]\n",
            "2it [00:00, 13.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.64\n",
            " Fscore: 0.63\n",
            " Precision: 0.72\n",
            " Recall: 0.68\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:05\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:04, 12.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.63\n",
            " Fscore: 0.62\n",
            " Precision: 0.71\n",
            " Recall: 0.68\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:05\n",
            "0.6297405313039061 0\n",
            "Saving model\n",
            "Saved/birnnscrat_lstm_64_2_100.pth\n",
            "best_val_fscore 0.6297405313039061\n",
            "best_test_fscore 0.6239978182306563\n",
            "best_val_rocauc 0\n",
            "best_test_rocauc 0\n",
            "best_val_precision 0.7160000851861317\n",
            "best_test_precision 0.7072098270640813\n",
            "best_val_recall 0.6821077047898096\n",
            "best_test_recall 0.6755333443816878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "path_file='best_model_json/bestModel_birnnscrat.json'\n",
        "with open(path_file,mode='r') as f:\n",
        "    params = json.load(f)\n",
        "for key in params:\n",
        "    if params[key] == 'True':\n",
        "          params[key]=True\n",
        "    elif params[key] == 'False':\n",
        "          params[key]=False\n",
        "    if( key in ['batch_size','num_classes','hidden_size','supervised_layer_pos','num_supervised_heads','random_seed','max_length']):\n",
        "        if(params[key]!='N/A'):\n",
        "            params[key]=int(params[key])\n",
        "        \n",
        "    if((key == 'weights') and (params['auto_weights']==False)):\n",
        "        params[key] = ast.literal_eval(params[key])\n",
        "\n",
        "##### change in logging to output the results to neptune\n",
        "params['logging']='local'\n",
        "params['device']='cpu'\n",
        "params['best_params']=False\n",
        "\n",
        "if torch.cuda.is_available() and params['device']=='cuda':    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print('Since you dont want to use GPU, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "    \n",
        "    \n",
        "#### Few handy keys that you can directly change.\n",
        "params['variance']=1\n",
        "params['epochs']=1\n",
        "params['to_save']=True\n",
        "params['num_classes']=2\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "if(params['num_classes']==2 and (params['auto_weights']==False)):\n",
        "      params['weights']=[1.0,1.0]\n",
        "        \n",
        "#for att_lambda in [0.001,0.01,0.1,1,10,100]\n",
        "train_model(params,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORBj47ArF8-F",
        "outputId": "032c3eff-bf08-4d0f-a853-5eba59ea7ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 63/20148 [00:00<00:32, 624.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_data 20148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20148/20148 [00:30<00:00, 658.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_error: 0\n",
            "no_majority: 919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 1281/15383 [00:00<00:02, 6100.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15383/15383 [00:02<00:00, 6831.31it/s]\n",
            "  6%|▌         | 866/15383 [00:00<00:01, 8659.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(22236, 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15383/15383 [00:01<00:00, 8165.05it/s]\n",
            "100%|██████████| 1922/1922 [00:00<00:00, 8167.40it/s]\n",
            "100%|██████████| 1924/1924 [00:00<00:00, 7757.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total dataset size: 19229\n",
            "[1.0796857 0.8201194 1.1703163]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [02:56,  2.72it/s]\n",
            "2it [00:00, 12.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.6838341473046\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:33, 14.32it/s]\n",
            "2it [00:00, 13.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.65\n",
            " Fscore: 0.63\n",
            " Precision: 0.66\n",
            " Recall: 0.62\n",
            " Roc Auc: 0.81\n",
            " Test took: 0:00:34\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:04, 14.44it/s]\n",
            "2it [00:00, 15.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.61\n",
            " Fscore: 0.59\n",
            " Precision: 0.63\n",
            " Recall: 0.59\n",
            " Roc Auc: 0.78\n",
            " Test took: 0:00:04\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:04, 14.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.62\n",
            " Fscore: 0.60\n",
            " Precision: 0.63\n",
            " Recall: 0.59\n",
            " Roc Auc: 0.78\n",
            " Test took: 0:00:04\n",
            "0.5908456676414833 0\n",
            "Saving model\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "best_val_fscore 0.5908456676414833\n",
            "best_test_fscore 0.5953800433596727\n",
            "best_val_rocauc 0.7813439473380873\n",
            "best_test_rocauc 0.7833791469223108\n",
            "best_val_precision 0.6323213563892517\n",
            "best_test_precision 0.6334081070577146\n",
            "best_val_recall 0.5854352674663518\n",
            "best_test_recall 0.5917058159561318\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "params['num_classes']=3\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "if(params['num_classes']==2 and (params['auto_weights']==False)):\n",
        "      params['weights']=[1.0,1.0]\n",
        "        \n",
        "#for att_lambda in [0.001,0.01,0.1,1,10,100]\n",
        "train_model(params,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRkvPokoMg3f",
        "outputId": "b07baef3-8671-4810-ab95-c051e350b873"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34jLhxdQ5stq",
        "outputId": "1e229253-9657-435d-a3b8-1357f40c86ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "Reading english - 1grams ...\n",
            "Since you dont want to use GPU, using the CPU instead.\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x5e92c000 @  0x7fc0775831e7 0x4b2590 0x5ad01c 0x5e46ad 0x58f90f 0x59172f 0x591ac9 0x4fc06a 0x4fc808 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b575e 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fc077180c87 0x5b636a\n",
            "tcmalloc: large alloc 2300993536 bytes == 0xe7b92000 @  0x7fc0775831e7 0x4b2590 0x5ad01c 0x4fc81a 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b575e 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fc077180c87 0x5b636a\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=['hatespeech' 'normal' 'offensive'], y=['normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  FutureWarning)\n",
            "total_data 1184\n",
            "100% 1184/1184 [00:01<00:00, 637.26it/s]\n",
            "100% 1184/1184 [00:00<00:00, 8463.09it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "Running eval on test data...\n",
            "100% 37/37 [00:02<00:00, 14.48it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " Accuracy: 0.461\n",
            " Fscore: 0.378\n",
            " Precision: 0.500\n",
            " Recall: 0.304\n",
            " Test took: 0:00:03\n",
            "100% 1184/1184 [00:00<00:00, 3621.94it/s]\n",
            "Since you dont want to use GPU, using the CPU instead.\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x5e92c000 @  0x7fc0775831e7 0x4b2590 0x5ad01c 0x5e46ad 0x58f90f 0x59172f 0x591ac9 0x4fc06a 0x4fc808 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b575e 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fc077180c87 0x5b636a\n",
            "tcmalloc: large alloc 2300993536 bytes == 0xe7b92000 @  0x7fc0775831e7 0x4b2590 0x5ad01c 0x4fc81a 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b575e 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fc077180c87 0x5b636a\n",
            "100% 1184/1184 [00:00<00:00, 6280.10it/s]\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "Running eval on test data...\n",
            "100% 37/37 [00:02<00:00, 14.47it/s]\n",
            "100% 1184/1184 [00:00<00:00, 1748.72it/s]\n",
            "Since you dont want to use GPU, using the CPU instead.\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x5e92c000 @  0x7fc0775831e7 0x4b2590 0x5ad01c 0x5e46ad 0x58f90f 0x59172f 0x591ac9 0x4fc06a 0x4fc808 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b575e 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fc077180c87 0x5b636a\n",
            "tcmalloc: large alloc 2300993536 bytes == 0xe7b92000 @  0x7fc0775831e7 0x4b2590 0x5ad01c 0x4fc81a 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b575e 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fc077180c87 0x5b636a\n",
            "100% 1184/1184 [00:00<00:00, 6264.50it/s]\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "Running eval on test data...\n",
            "100% 37/37 [00:02<00:00, 14.64it/s]\n",
            "\u001b[0m/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "Reading english - 1grams ...\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x5ea0a000 @  0x7fc5d12f11e7 0x4b2590 0x5ad01c 0x5e46ad 0x58f90f 0x59172f 0x591ac9 0x4fc06a 0x4fc808 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fc5d0eeec87 0x5b636a\n",
            "tcmalloc: large alloc 2300993536 bytes == 0xe7c70000 @  0x7fc5d12f11e7 0x4b2590 0x5ad01c 0x4fc81a 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fc5d0eeec87 0x5b636a\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=['non-toxic' 'toxic'], y=['non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  FutureWarning)\n",
            "total_data 1966\n",
            "100% 1966/1966 [00:02<00:00, 688.35it/s]\n",
            "100% 1966/1966 [00:00<00:00, 8450.80it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Saved/birnnscrat_lstm_64_2_100.pth\n",
            "Running eval on test data...\n",
            "100% 62/62 [00:04<00:00, 14.92it/s]\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python testing_with_rational.py birnn_scrat 100\n",
        "!python testing_for_bias.py birnn_scrat 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SynOxIW5PY-v",
        "outputId": "43436ac0-4738-4928-c367-52f0cb6bbf67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestModel_birnnscrat_100_explanation_top5.json\tbestModel_birnnscrat_bias.json\n"
          ]
        }
      ],
      "source": [
        "!ls explanations_dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1-wjKhvNrF5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Bias Calculation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GAPagtSDQ9zo"
      },
      "outputs": [],
      "source": [
        "from collections import Counter,defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lFOmyTVIRJ7R"
      },
      "outputs": [],
      "source": [
        "# get_annotated_data method is used to load the dataset\n",
        "from Preprocess.dataCollect import get_annotated_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkPyAXgDPqDh",
        "outputId": "ec5d9ba7-d25c-440b-99be-4004008069ba"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HateXplain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iw2qaVXLROW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30a2ee2-72d1-43dd-87cd-ad49804dc8af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20199"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "dict_data_folder={\n",
        "      '2':{'data_file':'/content/HateXplain/Data/new1_dataset.json','class_label':'Data/classes_two.npy'},\n",
        "      '3':{'data_file':'/content/HateXplain/Data/new1_dataset.json','class_label':'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "params = {}\n",
        "\n",
        "# We need to load the dataset with the labels as 'toxic' and 'non-toxic'. \n",
        "# We consider hatespeech and offensive as toxic and normal as non-toxic.\n",
        "params['num_classes']=2  \n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "data_all_labelled=get_annotated_data(params)\n",
        "len(data_all_labelled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LDRrrN8CRRfg",
        "outputId": "e92a13d6-8375-4bcc-9f1d-2577a324b5ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           post_id  \\\n",
              "0      1179055004553900032_twitter   \n",
              "1      1179063826874032128_twitter   \n",
              "2      1178793830532956161_twitter   \n",
              "3      1179088797964763136_twitter   \n",
              "4      1179085312976445440_twitter   \n",
              "...                            ...   \n",
              "20194   202277948702144406_tweeter   \n",
              "20195   202253115627130147_tweeter   \n",
              "20196   202257195143182238_tweeter   \n",
              "20197   202221302227592626_tweeter   \n",
              "20198   202217154743326539_tweeter   \n",
              "\n",
              "                                                    text annotatorid1  \\\n",
              "0      [i, dont, think, im, getting, my, baby, them, ...            1   \n",
              "1      [we, cannot, continue, calling, ourselves, fem...            1   \n",
              "2                    [nawt, yall, niggers, ignoring, me]            4   \n",
              "3      [<user>, i, am, bit, confused, coz, chinese, p...            1   \n",
              "4      [this, bitch, in, whataburger, eating, a, burg...            4   \n",
              "...                                                  ...          ...   \n",
              "20194  [, His, fake, rap, is, nauseating, He, s, a, M...       Abreen   \n",
              "20195  [, Brother, Nature, ain, t, black, no, more, h...       Abreen   \n",
              "20196  [, Teddy, fat, sorry, ass, wrong, for, how, he...       Abreen   \n",
              "20197  [, Go, back, to, Guatemala, you, stinking, wet...       Abreen   \n",
              "20198  [, Communist, wetback, douchebag, is, such, a,...       Abreen   \n",
              "\n",
              "                  target1       label1 annotatorid2             target2  \\\n",
              "0                  [None]       normal            2              [None]   \n",
              "1                  [None]       normal            2              [None]   \n",
              "2               [African]       normal            2              [None]   \n",
              "3                 [Asian]   hatespeech            4             [Asian]   \n",
              "4      [Caucasian, Women]   hatespeech            2  [Women, Caucasian]   \n",
              "...                   ...          ...          ...                 ...   \n",
              "20194           [African]  hatespeech         Clary           [African]   \n",
              "20195                  []  hatespeech         Clary                  []   \n",
              "20196                  []  hatespeech         Clary                  []   \n",
              "20197           [Refugee]  hatespeech         Clary           [Refugee]   \n",
              "20198           [Refugee]  hatespeech         Clary           [Refugee]   \n",
              "\n",
              "           label2 annotatorid3             target3      label3  \\\n",
              "0          normal            3              [None]      normal   \n",
              "1          normal            3              [None]      normal   \n",
              "2          normal            3           [African]  hatespeech   \n",
              "3       offensive            3             [Asian]  hatespeech   \n",
              "4      hatespeech            3  [Women, Caucasian]   offensive   \n",
              "...           ...          ...                 ...         ...   \n",
              "20194  hatespeech          Lei           [African]  hatespeech   \n",
              "20195  hatespeech          Lei                  []      normal   \n",
              "20196  hatespeech          Lei             [other]  hatespeech   \n",
              "20197  hatespeech          Lei           [Refugee]  hatespeech   \n",
              "20198  hatespeech          Lei           [Refugee]  hatespeech   \n",
              "\n",
              "                                              rationales final_label  \n",
              "0                                                     []   non-toxic  \n",
              "1                                                     []   non-toxic  \n",
              "2                                                     []   non-toxic  \n",
              "3      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       toxic  \n",
              "4      [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       toxic  \n",
              "...                                                  ...         ...  \n",
              "20194  [[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,...       toxic  \n",
              "20195  [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   undecided  \n",
              "20196  [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       toxic  \n",
              "20197  [[0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 1, 1],...       toxic  \n",
              "20198  [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...       toxic  \n",
              "\n",
              "[20199 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-45d97bef-96a7-4419-92a2-feebfbe02568\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>text</th>\n",
              "      <th>annotatorid1</th>\n",
              "      <th>target1</th>\n",
              "      <th>label1</th>\n",
              "      <th>annotatorid2</th>\n",
              "      <th>target2</th>\n",
              "      <th>label2</th>\n",
              "      <th>annotatorid3</th>\n",
              "      <th>target3</th>\n",
              "      <th>label3</th>\n",
              "      <th>rationales</th>\n",
              "      <th>final_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1179055004553900032_twitter</td>\n",
              "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>2</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>3</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1179063826874032128_twitter</td>\n",
              "      <td>[we, cannot, continue, calling, ourselves, fem...</td>\n",
              "      <td>1</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>2</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>3</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1178793830532956161_twitter</td>\n",
              "      <td>[nawt, yall, niggers, ignoring, me]</td>\n",
              "      <td>4</td>\n",
              "      <td>[African]</td>\n",
              "      <td>normal</td>\n",
              "      <td>2</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>3</td>\n",
              "      <td>[African]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1179088797964763136_twitter</td>\n",
              "      <td>[&lt;user&gt;, i, am, bit, confused, coz, chinese, p...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>4</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>3</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1179085312976445440_twitter</td>\n",
              "      <td>[this, bitch, in, whataburger, eating, a, burg...</td>\n",
              "      <td>4</td>\n",
              "      <td>[Caucasian, Women]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>2</td>\n",
              "      <td>[Women, Caucasian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>3</td>\n",
              "      <td>[Women, Caucasian]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20194</th>\n",
              "      <td>202277948702144406_tweeter</td>\n",
              "      <td>[, His, fake, rap, is, nauseating, He, s, a, M...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[African]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[African]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[African]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20195</th>\n",
              "      <td>202253115627130147_tweeter</td>\n",
              "      <td>[, Brother, Nature, ain, t, black, no, more, h...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>undecided</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20196</th>\n",
              "      <td>202257195143182238_tweeter</td>\n",
              "      <td>[, Teddy, fat, sorry, ass, wrong, for, how, he...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[other]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20197</th>\n",
              "      <td>202221302227592626_tweeter</td>\n",
              "      <td>[, Go, back, to, Guatemala, you, stinking, wet...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[Refugee]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[Refugee]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[Refugee]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 1, 1],...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20198</th>\n",
              "      <td>202217154743326539_tweeter</td>\n",
              "      <td>[, Communist, wetback, douchebag, is, such, a,...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[Refugee]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[Refugee]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[Refugee]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20199 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45d97bef-96a7-4419-92a2-feebfbe02568')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-45d97bef-96a7-4419-92a2-feebfbe02568 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-45d97bef-96a7-4419-92a2-feebfbe02568');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "data_all_labelled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "14K9-nzFRU8B"
      },
      "outputs": [],
      "source": [
        "def generate_target_information(dataset):\n",
        "    final_target_output = defaultdict(list)\n",
        "    all_communities_selected = []\n",
        "    \n",
        "    for each in dataset.iterrows(): \n",
        "        # All the target communities tagged for this post\n",
        "        all_targets = each[1]['target1']+each[1]['target2']+each[1]['target3']  \n",
        "        community_dict = dict(Counter(all_targets))\n",
        "        \n",
        "        # Select only those communities which are present more than once.\n",
        "        for key in community_dict:\n",
        "            if community_dict[key]>1:  \n",
        "                final_target_output[each[1]['post_id']].append(key)\n",
        "                all_communities_selected.append(key)\n",
        "        \n",
        "        # If no community is selected based on majority voting then we don't select any community\n",
        "        if each[1]['post_id'] not in final_target_output:\n",
        "            final_target_output[each[1]['post_id']].append('None')\n",
        "            all_communities_selected.append(key)\n",
        "\n",
        "    return final_target_output, all_communities_selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mEmm7AD9Ra13"
      },
      "outputs": [],
      "source": [
        "target_information, all_communities_selected = generate_target_information(data_all_labelled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPCh_pj3ReFA",
        "outputId": "bf32859b-edec-4d66-f2de-855f1a1e154e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['African',\n",
              " 'Islam',\n",
              " 'Jewish',\n",
              " 'Homosexual',\n",
              " 'Women',\n",
              " 'Refugee',\n",
              " 'Arab',\n",
              " 'Caucasian',\n",
              " 'Asian',\n",
              " 'Hispanic']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "community_count_dict = Counter(all_communities_selected)\n",
        "\n",
        "# We remove None and Other from dictionary\n",
        "community_count_dict.pop('None')\n",
        "community_count_dict.pop('Other')\n",
        "\n",
        "# For the bias calculation, we are considering the top 10 communites based on their count\n",
        "list_selected_community = [community for community, value in community_count_dict.most_common(10)]\n",
        "list_selected_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "eRU3peSrRhVo"
      },
      "outputs": [],
      "source": [
        "# Based on the top 10 communities, we filter the target_information\n",
        "# This will remove the other communities from the calculation\n",
        "\n",
        "final_target_information ={}\n",
        "for each in target_information:\n",
        "    temp = list(set(target_information[each])&set(list_selected_community))\n",
        "    if len(temp) == 0:\n",
        "        final_target_information[each] = None\n",
        "    else:\n",
        "        final_target_information[each] = temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BEsvl0xaRkpw"
      },
      "outputs": [],
      "source": [
        "# Add a new column 'final_target_category' which will contain the selected target community names\n",
        "data_all_labelled['final_target_category'] = data_all_labelled['post_id'].map(final_target_information)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L0-Y6iMiRnxH",
        "outputId": "815dda7e-6f82-4e1a-a581-f83d881eefa1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           post_id  \\\n",
              "0      1179055004553900032_twitter   \n",
              "5                     13851720_gab   \n",
              "17     1178818409812746240_twitter   \n",
              "25                    19346774_gab   \n",
              "36     1165819983701643266_twitter   \n",
              "...                            ...   \n",
              "20193   202223249403682616_tweeter   \n",
              "20194   202277948702144406_tweeter   \n",
              "20196   202257195143182238_tweeter   \n",
              "20197   202221302227592626_tweeter   \n",
              "20198   202217154743326539_tweeter   \n",
              "\n",
              "                                                    text annotatorid1  \\\n",
              "0      [i, dont, think, im, getting, my, baby, them, ...            1   \n",
              "5      [laura, loomer, raped, me, while, screaming, a...            1   \n",
              "17     [<user>, what, did, the, old, lady, do, was, s...            9   \n",
              "25     [as, much, as, i, appreciate, a, format, of, o...            9   \n",
              "36     [sex, be, so, good, a, bitch, be, slow, stroki...            4   \n",
              "...                                                  ...          ...   \n",
              "20193  [, Green, tea, You, muzzie, Stop, your, taqqiy...       Abreen   \n",
              "20194  [, His, fake, rap, is, nauseating, He, s, a, M...       Abreen   \n",
              "20196  [, Teddy, fat, sorry, ass, wrong, for, how, he...       Abreen   \n",
              "20197  [, Go, back, to, Guatemala, you, stinking, wet...       Abreen   \n",
              "20198  [, Communist, wetback, douchebag, is, such, a,...       Abreen   \n",
              "\n",
              "         target1       label1 annotatorid2    target2      label2  \\\n",
              "0         [None]       normal            2     [None]      normal   \n",
              "5       [Jewish]   hatespeech            2   [Jewish]  hatespeech   \n",
              "17        [None]       normal           10     [None]      normal   \n",
              "25        [None]       normal           13     [None]      normal   \n",
              "36       [Women]    offensive            7    [Women]   offensive   \n",
              "...          ...          ...          ...        ...         ...   \n",
              "20193    [Asian]  hatespeech         Clary    [Asian]   offensive   \n",
              "20194  [African]  hatespeech         Clary  [African]  hatespeech   \n",
              "20196         []  hatespeech         Clary         []  hatespeech   \n",
              "20197  [Refugee]  hatespeech         Clary  [Refugee]  hatespeech   \n",
              "20198  [Refugee]  hatespeech         Clary  [Refugee]  hatespeech   \n",
              "\n",
              "      annotatorid3     target3      label3  \\\n",
              "0                3      [None]      normal   \n",
              "5                3    [Jewish]  hatespeech   \n",
              "17               4      [None]      normal   \n",
              "25               4  [Hispanic]   offensive   \n",
              "36              16      [None]      normal   \n",
              "...            ...         ...         ...   \n",
              "20193          Lei     [Asian]   offensive   \n",
              "20194          Lei   [African]  hatespeech   \n",
              "20196          Lei     [other]  hatespeech   \n",
              "20197          Lei   [Refugee]  hatespeech   \n",
              "20198          Lei   [Refugee]  hatespeech   \n",
              "\n",
              "                                              rationales final_label  \\\n",
              "0                                                     []   non-toxic   \n",
              "5      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,...       toxic   \n",
              "17                                                    []   non-toxic   \n",
              "25                                                    []   non-toxic   \n",
              "36     [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, ...       toxic   \n",
              "...                                                  ...         ...   \n",
              "20193  [[0, 0, 0, 1, 0, 0, 0, 0], [1, 1, 0, 1, 0, 0, ...       toxic   \n",
              "20194  [[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,...       toxic   \n",
              "20196  [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       toxic   \n",
              "20197  [[0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 1, 1],...       toxic   \n",
              "20198  [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...       toxic   \n",
              "\n",
              "      final_target_category  \n",
              "0                      None  \n",
              "5                  [Jewish]  \n",
              "17                     None  \n",
              "25                     None  \n",
              "36                  [Women]  \n",
              "...                     ...  \n",
              "20193               [Asian]  \n",
              "20194             [African]  \n",
              "20196                  None  \n",
              "20197             [Refugee]  \n",
              "20198             [Refugee]  \n",
              "\n",
              "[1966 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-674aff42-68fa-41ea-b25f-ae77b94fd061\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>text</th>\n",
              "      <th>annotatorid1</th>\n",
              "      <th>target1</th>\n",
              "      <th>label1</th>\n",
              "      <th>annotatorid2</th>\n",
              "      <th>target2</th>\n",
              "      <th>label2</th>\n",
              "      <th>annotatorid3</th>\n",
              "      <th>target3</th>\n",
              "      <th>label3</th>\n",
              "      <th>rationales</th>\n",
              "      <th>final_label</th>\n",
              "      <th>final_target_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1179055004553900032_twitter</td>\n",
              "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>2</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>3</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>13851720_gab</td>\n",
              "      <td>[laura, loomer, raped, me, while, screaming, a...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Jewish]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>2</td>\n",
              "      <td>[Jewish]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>3</td>\n",
              "      <td>[Jewish]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Jewish]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1178818409812746240_twitter</td>\n",
              "      <td>[&lt;user&gt;, what, did, the, old, lady, do, was, s...</td>\n",
              "      <td>9</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>10</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>4</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>19346774_gab</td>\n",
              "      <td>[as, much, as, i, appreciate, a, format, of, o...</td>\n",
              "      <td>9</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>13</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>4</td>\n",
              "      <td>[Hispanic]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1165819983701643266_twitter</td>\n",
              "      <td>[sex, be, so, good, a, bitch, be, slow, stroki...</td>\n",
              "      <td>4</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>7</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>16</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, ...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Women]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20193</th>\n",
              "      <td>202223249403682616_tweeter</td>\n",
              "      <td>[, Green, tea, You, muzzie, Stop, your, taqqiy...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[[0, 0, 0, 1, 0, 0, 0, 0], [1, 1, 0, 1, 0, 0, ...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Asian]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20194</th>\n",
              "      <td>202277948702144406_tweeter</td>\n",
              "      <td>[, His, fake, rap, is, nauseating, He, s, a, M...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[African]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[African]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[African]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[African]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20196</th>\n",
              "      <td>202257195143182238_tweeter</td>\n",
              "      <td>[, Teddy, fat, sorry, ass, wrong, for, how, he...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[other]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20197</th>\n",
              "      <td>202221302227592626_tweeter</td>\n",
              "      <td>[, Go, back, to, Guatemala, you, stinking, wet...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[Refugee]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[Refugee]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[Refugee]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 1, 1],...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Refugee]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20198</th>\n",
              "      <td>202217154743326539_tweeter</td>\n",
              "      <td>[, Communist, wetback, douchebag, is, such, a,...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[Refugee]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[Refugee]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[Refugee]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Refugee]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1966 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-674aff42-68fa-41ea-b25f-ae77b94fd061')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-674aff42-68fa-41ea-b25f-ae77b94fd061 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-674aff42-68fa-41ea-b25f-ae77b94fd061');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.\n",
        "postpost_id_divisions_path = '/content/HateXplain/Data/final1_post_id_divisions.json'\n",
        "\n",
        "with open(postpost_id_divisions_path, 'r') as fp:\n",
        "    post_id_dict=json.load(fp)\n",
        "\n",
        "data_all_labelled_bias = data_all_labelled[data_all_labelled['post_id'].isin(post_id_dict['test'])]\n",
        "data_all_labelled_bias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_all_labelled_bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLj9nT9WGKBZ",
        "outputId": "2e0fe973-26fd-43f1-ec6b-f53f293f4632"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1966"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "uT3uzWgcRqaf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# The names of the files which will contain the bias output for each model\n",
        "bias_score_file_mapping={\n",
        "    'BiRNN-Attn':'bestModel_birnnscrat_bias.json',\n",
        "}\n",
        "\n",
        "# The parent folder path of the bias output files\n",
        "parent_path = './explanations_dicts/'\n",
        "\n",
        "# The bias methods that will be considered\n",
        "method_list = ['subgroup', 'bpsn', 'bnsp']\n",
        "\n",
        "community_list = list(list_selected_community)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "X0H9nMZNR1v3"
      },
      "outputs": [],
      "source": [
        "# This function is used to convert the classification into a [0-1] score \n",
        "# with a value of 0 meaning non-toxic and 1 meaning toxic\n",
        "def convert_to_score(label_name, label_dict):\n",
        "    if label_name=='non-toxic':\n",
        "        return 1-label_dict[label_name]\n",
        "    else:\n",
        "        return label_dict[label_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qUFyY8iVR4kn"
      },
      "outputs": [],
      "source": [
        "# Function to divide the ids into postive or class class based on the method.\n",
        "def bias_evaluation_metric(dataset, method, community):\n",
        "    positive_ids = []\n",
        "    negative_ids = []\n",
        "    if method=='subgroup':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "            else:\n",
        "                pass\n",
        "    elif method=='bpsn':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "#                 print(eachrow[1]['final_label'])\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "            else:\n",
        "                if eachrow[1]['final_label'] !='non-toxic':\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "    elif method=='bnsp':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "                if eachrow[1]['final_label'] !='non-toxic':\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "            else:\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "    else:\n",
        "        print('Incorrect option selected!!!')\n",
        "                \n",
        "    return {'positiveID':positive_ids, 'negativeID':negative_ids}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "e7672bee7a984df2ab0abaa0a78a9b02",
            "8ec318b9e72144ea8f652424ac25cfd7",
            "562075d81d5c44fead1c4782240dc752",
            "6d780bb60c3645ad8b77efac530a3647",
            "a1db63506d8e4dab9fd497139c92580f",
            "7db486634ca9441a84b04fa307ef6edf",
            "1f86dfbc2e4348edbc25d0be264bc325",
            "b970d85bcf72428db276ecd862555b19"
          ]
        },
        "id": "_o-CBxRFR7YQ",
        "outputId": "484063ec-636a-4a92-aa18-938ae45e2af7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7672bee7a984df2ab0abaa0a78a9b02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "final_bias_dictionary = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "# We load each of the model bias output file and compute the bias score using each method for all the community\n",
        "for each_model in tqdm(bias_score_file_mapping):\n",
        "    total_data ={}\n",
        "    with open(parent_path+bias_score_file_mapping[each_model]) as fp:\n",
        "        for line in fp:\n",
        "            data = json.loads(line)\n",
        "            total_data[data['annotation_id']] = data\n",
        "    for each_method in method_list:\n",
        "        for each_community in community_list:\n",
        "            community_data = bias_evaluation_metric(data_all_labelled_bias, each_method, each_community)\n",
        "            truth_values = []\n",
        "            prediction_values = []\n",
        "\n",
        "\n",
        "            label_to_value = {'toxic':1.0, 'non-toxic':0.0}\n",
        "            for each in community_data['positiveID']:\n",
        "                truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
        "                prediction_values.append(convert_to_score(total_data[each]['classification'], total_data[each]['classification_scores']))\n",
        "\n",
        "            for each in community_data['negativeID']:\n",
        "                truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
        "                prediction_values.append(convert_to_score(total_data[each]['classification'], total_data[each]['classification_scores']))\n",
        "\n",
        "            roc_output_value = roc_auc_score(truth_values, prediction_values)\n",
        "            final_bias_dictionary[each_model][each_method][each_community] = roc_output_value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(total_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUcLIMyhBzwT",
        "outputId": "52e7f30b-288c-4ce7-a6f2-fa00aab09cd6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1966"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_uoz4r8JR-23",
        "outputId": "e47cd1c0-4802-42cd-8e3a-76c48712c101"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'%.4f'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "%precision 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PIQi-Am9SCIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2cb3562-6d05-4551-b521-18b1e81f3d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiRNN-Attn subgroup 0.6715712998436169\n",
            "BiRNN-Attn bpsn 0.597499949769292\n",
            "BiRNN-Attn bnsp 0.6317074745837676\n"
          ]
        }
      ],
      "source": [
        "# To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:\n",
        "power_value = -5\n",
        "num_communities = len(community_list)\n",
        "\n",
        "for each_model in final_bias_dictionary:\n",
        "    for each_method in final_bias_dictionary[each_model]:\n",
        "        temp_value =[]\n",
        "        for each_community in final_bias_dictionary[each_model][each_method]:\n",
        "            temp_value.append(pow(final_bias_dictionary[each_model][each_method][each_community], power_value))\n",
        "        print(each_model, each_method, pow(np.sum(temp_value)/num_communities, 1/power_value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cos2FyRyScI6"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Explainability Calculation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "L0n04ccES0G3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import more_itertools as mit\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "DQ37OLVZS8gB"
      },
      "outputs": [],
      "source": [
        "# get_annotated_data method is used to load the dataset\n",
        "from Preprocess import *\n",
        "from Preprocess.dataCollect import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8AZCJG3wS-ko"
      },
      "outputs": [],
      "source": [
        "dict_data_folder={\n",
        "      '2':{'data_file':'/content/HateXplain/Data/new1_dataset.json','class_label':'Data/classes_two.npy'},\n",
        "      '3':{'data_file':'/content/HateXplain/Data/new1_dataset.json','class_label':'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "# We need to load the dataset with the labels as 'hatespeech', 'offensive', and 'normal' (3-class). \n",
        "\n",
        "params = {}\n",
        "params['num_classes']=3\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "data_all_labelled=get_annotated_data(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5DLppxPTAjo",
        "outputId": "ebdb78cb-f6f6-408b-e415-1c4e014e2598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Normal tokenizer...\n"
          ]
        }
      ],
      "source": [
        "# The important key here is the 'bert_token'. Set it to True for Bert based models and False for Others.\n",
        "\n",
        "params_data={\n",
        "    'include_special':False,  #True is want to include <url> in place of urls if False will be removed\n",
        "    'bert_tokens':False, #True /False\n",
        "    'type_attention':'softmax', #softmax\n",
        "    'set_decay':0.1,\n",
        "    'majority':2,\n",
        "    'max_length':128,\n",
        "    'variance':10,\n",
        "    'window':4,\n",
        "    'alpha':0.5,\n",
        "    'p_value':0.8,\n",
        "    'method':'additive',\n",
        "    'decay':False,\n",
        "    'normalized':False,\n",
        "    'not_recollect':True,\n",
        "}\n",
        "\n",
        "\n",
        "if(params_data['bert_tokens']):\n",
        "    print('Loading BERT tokenizer...')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
        "else:\n",
        "    print('Loading Normal tokenizer...')\n",
        "    tokenizer=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Aqwkmy9ITEvH"
      },
      "outputs": [],
      "source": [
        "# Load the whole dataset and get the tokenwise rationales\n",
        "def get_training_data(data):\n",
        "    post_ids_list=[]\n",
        "    text_list=[]\n",
        "    attention_list=[]\n",
        "    label_list=[]\n",
        "    \n",
        "    final_binny_output = []\n",
        "    print('total_data',len(data))\n",
        "    for index,row in tqdm(data.iterrows(),total=len(data)):\n",
        "        annotation=row['final_label']\n",
        "        \n",
        "        text=row['text']\n",
        "        post_id=row['post_id']\n",
        "        annotation_list=[row['label1'],row['label2'],row['label3']]\n",
        "        tokens_all = list(row['text'])\n",
        "#         attention_masks =  [list(row['explain1']),list(row['explain2']),list(row['explain1'])]\n",
        "        \n",
        "        if(annotation!= 'undecided'):\n",
        "            tokens_all,attention_masks=returnMask(row, params_data, tokenizer)\n",
        "            final_binny_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])\n",
        "\n",
        "    return final_binny_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2lkIo1ATHjH",
        "outputId": "64e43b09-68de-4eb2-d802-d9eab46a4d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 65/20199 [00:00<00:31, 641.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_data 20199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20199/20199 [00:33<00:00, 601.12it/s]\n"
          ]
        }
      ],
      "source": [
        "training_data=get_training_data(data_all_labelled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "QZxfQUvdTJzn"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list\n",
        "def find_ranges(iterable):\n",
        "    \"\"\"Yield range of consecutive numbers.\"\"\"\n",
        "    for group in mit.consecutive_groups(iterable):\n",
        "        group = list(group)\n",
        "        if len(group) == 1:\n",
        "            yield group[0]\n",
        "        else:\n",
        "            yield group[0], group[-1]\n",
        "            \n",
        "# Convert dataset into ERASER format: https://github.com/jayded/eraserbenchmark/blob/master/rationale_benchmark/utils.py\n",
        "def get_evidence(post_id, anno_text, explanations):\n",
        "    output = []\n",
        "\n",
        "    indexes = sorted([i for i, each in enumerate(explanations) if each==1])\n",
        "    span_list = list(find_ranges(indexes))\n",
        "\n",
        "    for each in span_list:\n",
        "        if type(each)== int:\n",
        "            start = each\n",
        "            end = each+1\n",
        "        elif len(each) == 2:\n",
        "            start = each[0]\n",
        "            end = each[1]+1\n",
        "        else:\n",
        "            print('error')\n",
        "\n",
        "        output.append({\"docid\":post_id, \n",
        "              \"end_sentence\": -1, \n",
        "              \"end_token\": end, \n",
        "              \"start_sentence\": -1, \n",
        "              \"start_token\": start, \n",
        "              \"text\": ' '.join([str(x) for x in anno_text[start:end]])})\n",
        "    return output\n",
        "\n",
        "# To use the metrices defined in ERASER, we will have to convert the dataset\n",
        "def convert_to_eraser_format(dataset, method, save_split, save_path, id_division):  \n",
        "    final_output = []\n",
        "    \n",
        "    if save_split:\n",
        "        train_fp = open(save_path+'train.jsonl', 'w')\n",
        "        val_fp = open(save_path+'val.jsonl', 'w')\n",
        "        test_fp = open(save_path+'test.jsonl', 'w')\n",
        "            \n",
        "    for tcount, eachrow in enumerate(dataset):\n",
        "        \n",
        "        temp = {}\n",
        "        post_id = eachrow[0]\n",
        "        post_class = eachrow[1]\n",
        "        anno_text_list = eachrow[2]\n",
        "        majority_label = eachrow[1]\n",
        "        \n",
        "        if majority_label=='normal':\n",
        "            continue\n",
        "        \n",
        "        all_labels = eachrow[4]\n",
        "        explanations = []\n",
        "        for each_explain in eachrow[3]:\n",
        "            explanations.append(list(each_explain))\n",
        "        \n",
        "        # For this work, we have considered the union of explanations. Other options could be explored as well.\n",
        "        if method == 'union':\n",
        "            final_explanation = [any(each) for each in zip(*explanations)]\n",
        "            final_explanation = [int(each) for each in final_explanation]\n",
        "        \n",
        "            \n",
        "        temp['annotation_id'] = post_id\n",
        "        temp['classification'] = post_class\n",
        "        temp['evidences'] = [get_evidence(post_id, list(anno_text_list), final_explanation)]\n",
        "        temp['query'] = \"What is the class?\"\n",
        "        temp['query_type'] = None\n",
        "        final_output.append(temp)\n",
        "        \n",
        "        if save_split:\n",
        "            if not os.path.exists(save_path+'docs'):\n",
        "                os.makedirs(save_path+'docs')\n",
        "            \n",
        "            with open(save_path+'docs/'+post_id, 'w') as fp:\n",
        "                fp.write(' '.join([str(x) for x in list(anno_text_list)]))\n",
        "            \n",
        "            if post_id in id_division['train']:\n",
        "                train_fp.write(json.dumps(temp)+'\\n')\n",
        "            \n",
        "            elif post_id in id_division['val']:\n",
        "                val_fp.write(json.dumps(temp)+'\\n')\n",
        "            \n",
        "            elif post_id in id_division['test']:\n",
        "                test_fp.write(json.dumps(temp)+'\\n')\n",
        "            else:\n",
        "                print(post_id)\n",
        "    \n",
        "    if save_split:\n",
        "        train_fp.close()\n",
        "        val_fp.close()\n",
        "        test_fp.close()\n",
        "        \n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "deYKnU3wTRJn"
      },
      "outputs": [],
      "source": [
        "# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.\n",
        "with open('./Data/final1_post_id_divisions.json') as fp:\n",
        "    id_division = json.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "XlA0iMeETjUd"
      },
      "outputs": [],
      "source": [
        "!mkdir ./Data/Evaluation\n",
        "!mkdir ./Data/Evaluation/Model_Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "B2XwjEPOTUaY"
      },
      "outputs": [],
      "source": [
        "method = 'union'\n",
        "save_split = True\n",
        "save_path = './Data/Evaluation/Model_Eval/'  #The dataset in Eraser Format will be stored here.\n",
        "output_eraser = convert_to_eraser_format(training_data, method, save_split, save_path, id_division)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "e76FcTICTXrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c260982d-2736-433d-fc26-dad88f5924cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docs  test.jsonl  train.jsonl  val.jsonl\n"
          ]
        }
      ],
      "source": [
        "!ls Data/Evaluation/Model_Eval/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67kj7CSHBEWv",
        "outputId": "17a04e95-664f-4540-ecf8-29103f870102"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_model_json\t\t\t     Models\n",
            "best_runs.sh\t\t\t     Parameters_description.md\n",
            "Bias_Calculation_NB.ipynb\t     parameters_selection.py\n",
            "convert_to_word2vec.py\t\t     Preprocess\n",
            "Data\t\t\t\t     __pycache__\n",
            "eraserbenchmark\t\t\t     README.md\n",
            "Example_HateExplain.ipynb\t     requirements.txt\n",
            "Explainability_Calculation_NB.ipynb  Saved\n",
            "explanations_dicts\t\t     TensorDataset\n",
            "Figures\t\t\t\t     testing_for_bias.py\n",
            "HateXplain\t\t\t     testing_with_lime.py\n",
            "LICENSE\t\t\t\t     testing_with_rational.py\n",
            "manual_training_inference.py\t     test_parallel.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd eraserbenchmark/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR973lE2BEaK",
        "outputId": "be9b0fb3-5e63-4735-a0fb-a17d1b815932"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HateXplain/eraserbenchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z3BqZLFBEi_",
        "outputId": "6ab8c30f-2753-4d3b-c782-95ad52136c4b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_exploration.ipynb\tparams\t\t     README.md\t       requirements.txt\n",
            "LICENSE\t\t\trationale_benchmark  REPRODUCTION.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "GzZLhJf-U8Vf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11cee508-34e0-4db9-97be-82d954197307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1668 MainThread Error in instances: 0 instances fail validation: set()\n",
            "  3251 MainThread No sentence level predictions detected, skipping sentence-level diagnostic\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "{'classification_scores': {'accuracy': 0.46114864864864863,\n",
            "                           'aopc_thresholds': None,\n",
            "                           'comprehensiveness': 0.12690107161619676,\n",
            "                           'comprehensiveness_aopc': None,\n",
            "                           'comprehensiveness_aopc_points': None,\n",
            "                           'comprehensiveness_entropy': 0.08589997199845727,\n",
            "                           'comprehensiveness_kl': 0.23743301722805846,\n",
            "                           'prf': {'accuracy': 0.46114864864864863,\n",
            "                                   'hatespeech': {'f1-score': 0.68836291913215,\n",
            "                                                  'precision': 0.8880407124681934,\n",
            "                                                  'recall': 0.5619967793880838,\n",
            "                                                  'support': 621},\n",
            "                                   'macro avg': {'f1-score': 0.3778535530817148,\n",
            "                                                 'precision': 0.4999473182347394,\n",
            "                                                 'recall': 0.30396932314712327,\n",
            "                                                 'support': 1184},\n",
            "                                   'normal': {'f1-score': 0.0,\n",
            "                                              'precision': 0.0,\n",
            "                                              'recall': 0.0,\n",
            "                                              'support': 0},\n",
            "                                   'offensive': {'f1-score': 0.44519774011299434,\n",
            "                                                 'precision': 0.6118012422360248,\n",
            "                                                 'recall': 0.34991119005328597,\n",
            "                                                 'support': 563},\n",
            "                                   'weighted avg': {'f1-score': 0.5727362335005751,\n",
            "                                                    'precision': 0.7566869778898901,\n",
            "                                                    'recall': 0.46114864864864863,\n",
            "                                                    'support': 1184}},\n",
            "                           'sufficiency': -0.008355937659035664,\n",
            "                           'sufficiency_aopc': None,\n",
            "                           'sufficiency_aopc_points': None,\n",
            "                           'sufficiency_entropy': 0.0314129382215655,\n",
            "                           'sufficiency_kl': 0.02101829980096085},\n",
            " 'iou_scores': [{'macro': {'f1': 0.22556572791882537,\n",
            "                           'p': 0.1466216216216215,\n",
            "                           'r': 0.4886824324324325},\n",
            "                 'micro': {'f1': 0.22288842544317,\n",
            "                           'p': 0.14563106796116504,\n",
            "                           'r': 0.47473625763464744},\n",
            "                 'threshold': 0.5}],\n",
            " 'rationale_prf': {'instance_macro': {'f1': 0.11539794664794659,\n",
            "                                      'p': 0.07818130630630679,\n",
            "                                      'r': 0.259712837837838},\n",
            "                   'instance_micro': {'f1': 0.11887382690302399,\n",
            "                                      'p': 0.07766990291262135,\n",
            "                                      'r': 0.2531926707384786}},\n",
            " 'token_prf': {'instance_macro': {'f1': 0.4933001129231857,\n",
            "                                  'p': 0.6004222972972975,\n",
            "                                  'r': 0.6260488686490909},\n",
            "               'instance_micro': {'f1': 0.43514231030437733,\n",
            "                                  'p': 0.6002384602282405,\n",
            "                                  'r': 0.3412744528374976}},\n",
            " 'token_soft_metrics': {'auprc': 0.8179723648640134,\n",
            "                        'average_precision': 0.8127921641724448,\n",
            "                        'roc_auc_score': 0.8343096840525286}}\n"
          ]
        }
      ],
      "source": [
        "!PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py --split test  --data_dir ../Data/Evaluation/Model_Eval --results ../explanations_dicts/bestModel_birnnscrat_100_explanation_top5.json --score_file ../model_explain_output.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V59mqw02AlUP",
        "outputId": "e691bb56-c52d-4487-9f0a-676562e93adf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HateXplain/eraserbenchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "D1eQENR4VLp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ffcb2d-efd7-44d5-d663-20c58d1d3b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Plausibility\n",
            "IOU F1 : 0.22556572791882537\n",
            "Token F1 : 0.4933001129231857\n",
            "AUPRC : 0.8179723648640134\n",
            "\n",
            "Faithfulness\n",
            "Comprehensiveness : 0.12690107161619676\n",
            "Sufficiency -0.008355937659035664\n"
          ]
        }
      ],
      "source": [
        "# print the required results\n",
        "with open('../model_explain_output.json') as fp:\n",
        "    output_data = json.load(fp)\n",
        "\n",
        "print('\\nPlausibility')\n",
        "print('IOU F1 :', output_data['iou_scores'][0]['macro']['f1'])\n",
        "print('Token F1 :', output_data['token_prf']['instance_macro']['f1'])\n",
        "print('AUPRC :', output_data['token_soft_metrics']['auprc'])\n",
        "\n",
        "print('\\nFaithfulness')\n",
        "print('Comprehensiveness :', output_data['classification_scores']['comprehensiveness'])\n",
        "print('Sufficiency', output_data['classification_scores']['sufficiency'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7DtYg-yNAkET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "P-JVJuctN4EJ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuple_community = []\n",
        "for each_model in final_bias_dictionary:\n",
        "    # Select the metric which you want to view. Possible values are subgroup, bpsn, bnsp\n",
        "    each_method = 'bnsp' \n",
        "    \n",
        "    for each_community in final_bias_dictionary[each_model][each_method]:\n",
        "        tuple_community.append((each_model, each_community, final_bias_dictionary[each_model][each_method][each_community]))"
      ],
      "metadata": {
        "id": "MOB018lKN8oS"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_community_score = pd.DataFrame(tuple_community, columns=['Model', 'Community', 'AUCROC'])"
      ],
      "metadata": {
        "id": "FiJPk4f2N9J9"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.catplot(x=\"Community\", y=\"AUCROC\", hue=\"Model\",\n",
        "             data=df_community_score,\n",
        "                 legend=False,\n",
        "                kind=\"bar\");\n",
        "ax.set(ylim=(0.3, 1.0))\n",
        "ax.set_xticklabels(rotation=45, size=13, horizontalalignment='right')\n",
        "\n",
        "# sns.set(font_scale = 0.1)\n",
        "\n",
        "\n",
        "\n",
        "handles = ax._legend_data.values()\n",
        "labels = ax._legend_data.keys()\n",
        "\n",
        "ax.fig.legend(handles=handles, labels=labels, loc='upper right', ncol=3)\n",
        "ax.fig.subplots_adjust(top=0.92)\n",
        "\n",
        "ax.set(xlabel=\"\")  \n",
        "\n",
        "print(each_method)\n",
        "# plt.savefig('bias-'+each_method+'.pdf', dpi=300, transparent=True, bbox_inches='tight')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "OZf78gzsN_OR",
        "outputId": "32487a1c-6c0a-4d1b-9eb6-829d66080274"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bnsp\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e7672bee7a984df2ab0abaa0a78a9b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ec318b9e72144ea8f652424ac25cfd7",
              "IPY_MODEL_562075d81d5c44fead1c4782240dc752"
            ],
            "layout": "IPY_MODEL_6d780bb60c3645ad8b77efac530a3647"
          }
        },
        "8ec318b9e72144ea8f652424ac25cfd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1db63506d8e4dab9fd497139c92580f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7db486634ca9441a84b04fa307ef6edf",
            "value": 1
          }
        },
        "562075d81d5c44fead1c4782240dc752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f86dfbc2e4348edbc25d0be264bc325",
            "placeholder": "​",
            "style": "IPY_MODEL_b970d85bcf72428db276ecd862555b19",
            "value": " 1/1 [02:46&lt;00:00, 166.53s/it]"
          }
        },
        "6d780bb60c3645ad8b77efac530a3647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1db63506d8e4dab9fd497139c92580f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7db486634ca9441a84b04fa307ef6edf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "1f86dfbc2e4348edbc25d0be264bc325": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b970d85bcf72428db276ecd862555b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}