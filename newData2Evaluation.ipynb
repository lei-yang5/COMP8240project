{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lei-yang5/COMP8240project/blob/main/newData2Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Hqb1OvsXDR",
        "outputId": "e9524481-f713-4191-8837-ba5805ded222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'HateXplain' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/punyajoy/HateXplain.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FPAJpqcYVEM0"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l4M1_SHtDIm",
        "outputId": "5a9c911d-b66e-4c20-a2ea-d3b6207f78f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HateXplain\n"
          ]
        }
      ],
      "source": [
        "cd HateXplain/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vl3PF9alWjHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36443d39-eb8e-4cd8-f379-c3ccc505c1d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘Saved/’: File exists\n",
            "mkdir: cannot create directory ‘explanations_dicts/’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir Saved/\n",
        "!mkdir explanations_dicts/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU8P_dV5Wnk5",
        "outputId": "714a12ae-d366-4703-8f16-a8856d2a9164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-30 06:26:19--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2022-10-30 06:26:19--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
            "--2022-10-30 06:26:20--  https://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘Data/glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  5.00MB/s    in 5m 53s  \n",
            "\n",
            "2022-10-30 06:32:14 (5.08 MB/s) - ‘Data/glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip  -P Data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZi9IlH-W_83",
        "outputId": "53b1f2fa-addd-4551-9f60-e9bd078d4d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Data/glove.42B.300d.zip\n",
            "  inflating: Data/glove.42B.300d.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip Data/glove.42B.300d.zip -d Data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gofRAO-crNgI"
      },
      "outputs": [],
      "source": [
        "!rm Data/glove.42B.300d.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h0akw80ipBIu",
        "outputId": "a75f1745-3df4-4ea0-bf5e-08aaacf991ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: tqdm==4.43.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.43.0)\n",
            "Collecting Keras==2.3.1\n",
            "  Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
            "Requirement already satisfied: waiting==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: ekphrasis==0.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.5.1)\n",
            "Requirement already satisfied: pandas==1.0.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.0.3)\n",
            "Requirement already satisfied: transformers==2.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (2.5.1)\n",
            "Requirement already satisfied: lime==0.2.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.2.0.1)\n",
            "Collecting numpy==1.16.3\n",
            "  Using cached numpy-1.16.3-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n",
            "Requirement already satisfied: matplotlib==3.2.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (3.2.1)\n",
            "Requirement already satisfied: gensim==3.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (3.8.1)\n",
            "Requirement already satisfied: neptune_client==0.4.107 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.4.107)\n",
            "Requirement already satisfied: knockknock==0.1.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (0.1.7)\n",
            "Requirement already satisfied: torch==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (1.1.0)\n",
            "Requirement already satisfied: apex==0.9.10dev in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (0.9.10.dev0)\n",
            "Requirement already satisfied: dataclasses==0.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (0.6)\n",
            "Requirement already satisfied: GPUtil==1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (1.4.0)\n",
            "Requirement already satisfied: scikit_learn==0.23.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (0.23.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (6.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (6.1.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (0.4.6)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (5.5.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.3->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.3->-r requirements.txt (line 7)) (2022.5)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (3.8.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (0.0.53)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (1.25.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (2022.6.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (0.1.97)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime==0.2.0.1->-r requirements.txt (line 9)) (0.17.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 11)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 11)) (0.11.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 12)) (5.2.1)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (0.18.2)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (1.3.1)\n",
            "Requirement already satisfied: py3nvml in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (0.2.7)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (7.1.2)\n",
            "Requirement already satisfied: bravado in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (11.0.3)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (3.1.29)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (3.2.2)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (2.6.0)\n",
            "Requirement already satisfied: websocket-client>=0.35.0 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (1.4.1)\n",
            "Requirement already satisfied: yagmail>=0.11.214 in /usr/local/lib/python3.7/dist-packages (from knockknock==0.1.7->-r requirements.txt (line 14)) (0.15.293)\n",
            "Requirement already satisfied: python-telegram-bot in /usr/local/lib/python3.7/dist-packages (from knockknock==0.1.7->-r requirements.txt (line 14)) (13.14)\n",
            "Requirement already satisfied: matrix-client in /usr/local/lib/python3.7/dist-packages (from knockknock==0.1.7->-r requirements.txt (line 14)) (0.4.0)\n",
            "Requirement already satisfied: keyring in /usr/local/lib/python3.7/dist-packages (from knockknock==0.1.7->-r requirements.txt (line 14)) (23.9.3)\n",
            "Requirement already satisfied: twilio in /usr/local/lib/python3.7/dist-packages (from knockknock==0.1.7->-r requirements.txt (line 14)) (7.15.0)\n",
            "Requirement already satisfied: cryptacular in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (1.6.2)\n",
            "Requirement already satisfied: wtforms-recaptcha in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (0.3.2)\n",
            "Requirement already satisfied: wtforms in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (3.0.1)\n",
            "Requirement already satisfied: velruse>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (1.1.1)\n",
            "Requirement already satisfied: zope.sqlalchemy in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (1.6)\n",
            "Requirement already satisfied: pyramid-mailer in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (0.15.1)\n",
            "Requirement already satisfied: pyramid>1.1.2 in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.23.2->-r requirements.txt (line 19)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.23.2->-r requirements.txt (line 19)) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune_client==0.4.107->-r requirements.txt (line 13)) (4.1.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune_client==0.4.107->-r requirements.txt (line 13)) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune_client==0.4.107->-r requirements.txt (line 13)) (5.0.0)\n",
            "Requirement already satisfied: hupper>=1.5 in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (1.10.3)\n",
            "Requirement already satisfied: zope.interface>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (5.5.0)\n",
            "Requirement already satisfied: webob>=1.8.3 in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (1.8.7)\n",
            "Requirement already satisfied: zope.deprecation>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (4.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (57.4.0)\n",
            "Requirement already satisfied: translationstring>=0.4 in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (1.4)\n",
            "Requirement already satisfied: venusian>=1.0 in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (3.0.0)\n",
            "Requirement already satisfied: plaster in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (1.0)\n",
            "Requirement already satisfied: plaster-pastedeploy in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 8)) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 8)) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9)) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9)) (2.9.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9)) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9)) (2021.11.2)\n",
            "Requirement already satisfied: python3-openid in /usr/local/lib/python3.7/dist-packages (from velruse>=1.0.3->apex==0.9.10dev->-r requirements.txt (line 16)) (3.2.0)\n",
            "Requirement already satisfied: anykeystore in /usr/local/lib/python3.7/dist-packages (from velruse>=1.0.3->apex==0.9.10dev->-r requirements.txt (line 16)) (0.2)\n",
            "Requirement already satisfied: premailer in /usr/local/lib/python3.7/dist-packages (from yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14)) (3.10.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.5.1->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.5.1->-r requirements.txt (line 8)) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.29.0,>=1.28.4 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.5.1->-r requirements.txt (line 8)) (1.28.4)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.0.4)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.7/dist-packages (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.6)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.7/dist-packages (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (5.17.1)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (3.17.6)\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (4.3.3)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.0.0.post1)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (3.0.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (5.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (22.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (4.13.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (0.18.1)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (2.3)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.5.1)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (0.1.4)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (20.11.0)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.12)\n",
            "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.3.8)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (3.9.0)\n",
            "Requirement already satisfied: pbkdf2 in /usr/local/lib/python3.7/dist-packages (from cryptacular->apex==0.9.10dev->-r requirements.txt (line 16)) (1.3)\n",
            "Requirement already satisfied: cached-property>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from fqdn->jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.5.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis==0.5.1->-r requirements.txt (line 6)) (0.2.5)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from isoduration->jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.2.3)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (0.8.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (3.2.3)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.7/dist-packages (from keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (3.3.3)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.7/dist-packages (from SecretStorage>=3.2->keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (38.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (2.21)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from jaraco.classes->keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (9.0.0)\n",
            "Requirement already satisfied: PasteDeploy>=2.0 in /usr/local/lib/python3.7/dist-packages (from plaster-pastedeploy->pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (3.0.1)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.7/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14)) (1.2.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14)) (4.2.2)\n",
            "Requirement already satisfied: cssutils in /usr/local/lib/python3.7/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14)) (2.6.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14)) (4.9.1)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.7/dist-packages (from py3nvml->neptune_client==0.4.107->-r requirements.txt (line 13)) (0.13.0)\n",
            "Requirement already satisfied: repoze.sendmail>=4.1 in /usr/local/lib/python3.7/dist-packages (from pyramid-mailer->apex==0.9.10dev->-r requirements.txt (line 16)) (4.4.1)\n",
            "Requirement already satisfied: transaction in /usr/local/lib/python3.7/dist-packages (from pyramid-mailer->apex==0.9.10dev->-r requirements.txt (line 16)) (3.0.1)\n",
            "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14)) (3.6.3)\n",
            "Requirement already satisfied: tornado==6.1 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14)) (6.1)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14)) (1.5.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from python3-openid->velruse>=1.0.3->apex==0.9.10dev->-r requirements.txt (line 16)) (0.7.1)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.7/dist-packages (from wtforms->apex==0.9.10dev->-r requirements.txt (line 16)) (2.0.1)\n",
            "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9 in /usr/local/lib/python3.7/dist-packages (from zope.sqlalchemy->apex==0.9.10dev->-r requirements.txt (line 16)) (1.4.42)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9->zope.sqlalchemy->apex==0.9.10dev->-r requirements.txt (line 16)) (1.1.3.post0)\n",
            "Installing collected packages: numpy, Keras\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: Keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.16.3 which is incompatible.\n",
            "xarray 0.20.2 requires pandas>=1.1, but you have pandas 1.0.3 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.16.3 which is incompatible.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.1.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.1.0 which is incompatible.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires keras<2.9,>=2.8.0rc0, but you have keras 2.3.1 which is incompatible.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires numpy>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.16.3 which is incompatible.\n",
            "resampy 0.4.2 requires numpy>=1.17, but you have numpy 1.16.3 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.16.3 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.16.3 which is incompatible.\n",
            "prophet 1.1.1 requires pandas>=1.0.4, but you have pandas 1.0.3 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.16.3 which is incompatible.\n",
            "plotnine 0.8.0 requires pandas>=1.1.0, but you have pandas 1.0.3 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "numba 0.56.3 requires numpy<1.24,>=1.18, but you have numpy 1.16.3 which is incompatible.\n",
            "mizani 0.7.3 requires pandas>=1.1.0, but you have pandas 1.0.3 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.3 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.23 requires numpy>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "jax 0.3.23 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "gym 0.25.2 requires numpy>=1.18.0, but you have numpy 1.16.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0, but you have pandas 1.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.1 which is incompatible.\n",
            "fastai 2.7.9 requires torch<1.14,>=1.7, but you have torch 1.1.0 which is incompatible.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "cmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.16.3 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.16.3 which is incompatible.\n",
            "aesara 2.7.9 requires numpy>=1.17.0, but you have numpy 1.16.3 which is incompatible.\n",
            "aeppl 0.0.33 requires numpy>=1.18.1, but you have numpy 1.16.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Keras-2.3.1 numpy-1.16.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nlq-y7f7nxdP"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec('Data/glove.42B.300d.txt', 'Data/glove.42B.300d_w2v.txt')\n",
        "word2vecmodel1 = KeyedVectors.load_word2vec_format('Data/glove.42B.300d_w2v.txt', binary=False)\n",
        "word2vecmodel1.save(\"Data/word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrVyCVHdlWSc",
        "outputId": "203e6aa3-faa8-4392-be8f-c9c07a87d1e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import gc\n",
        "del word2vecmodel1\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfpYO5kshKMf",
        "outputId": "af959767-50a5-44b6-a73c-a92965adfdf2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.16.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.43.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install keras==2.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PrJCxwIhr3o",
        "outputId": "8156aa7f-e3ec-4059-fc52-14006cd94ae7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.8\n",
            "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires numpy>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.3 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1E2N1U2plHBA",
        "outputId": "abc1e80c-9a02-4ce2-eec9-b09e49553bf7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.8 in /usr/local/lib/python3.7/dist-packages (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.27.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.3.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (57.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (4.1.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (14.0.6)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.3.0)\n",
            "Collecting numpy>=1.20\n",
            "  Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.14.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.12)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.6.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.50.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.8) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.25.11)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.16.3\n",
            "    Uninstalling numpy-1.16.3:\n",
            "      Successfully uninstalled numpy-1.16.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "xarray 0.20.2 requires pandas>=1.1, but you have pandas 1.0.3 which is incompatible.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.1.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.1.0 which is incompatible.\n",
            "prophet 1.1.1 requires pandas>=1.0.4, but you have pandas 1.0.3 which is incompatible.\n",
            "plotnine 0.8.0 requires pandas>=1.1.0, but you have pandas 1.0.3 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "mizani 0.7.3 requires pandas>=1.1.0, but you have pandas 1.0.3 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.23 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0, but you have pandas 1.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.1 which is incompatible.\n",
            "fastai 2.7.9 requires torch<1.14,>=1.7, but you have torch 1.1.0 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5HS3NNHzmX64"
      },
      "outputs": [],
      "source": [
        "!rm Data/glove.42B.300d.txt\n",
        "!rm Data/glove.42B.300d_w2v.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53QcDc7AqkK5",
        "outputId": "5b52bdeb-c958-4c7c-a9d1-d6f35efe08be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading english - 1grams ...\n"
          ]
        }
      ],
      "source": [
        "from manual_training_inference import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIKH2h5hzwcT",
        "outputId": "16a20532-6562-4ac9-f7f1-be2862eaa42c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Since you dont want to use GPU, using the CPU instead.\n",
            "[1.2301791 0.8423818]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [03:00,  2.66it/s]\n",
            "2it [00:00, 14.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.310826164769\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:33, 14.21it/s]\n",
            "2it [00:00, 13.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.70\n",
            " Fscore: 0.70\n",
            " Precision: 0.74\n",
            " Recall: 0.73\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:34\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:04, 14.28it/s]\n",
            "2it [00:00, 14.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.66\n",
            " Fscore: 0.66\n",
            " Precision: 0.72\n",
            " Recall: 0.70\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:04\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:04, 14.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.67\n",
            " Fscore: 0.67\n",
            " Precision: 0.72\n",
            " Recall: 0.71\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:04\n",
            "0.6588550826112853 0\n",
            "Saving model\n",
            "Saved/birnnscrat_lstm_64_2_100.pth\n",
            "best_val_fscore 0.6588550826112853\n",
            "best_test_fscore 0.6681477560194566\n",
            "best_val_rocauc 0\n",
            "best_test_rocauc 0\n",
            "best_val_precision 0.7170125038941049\n",
            "best_test_precision 0.720009958776411\n",
            "best_val_recall 0.69933824923888\n",
            "best_test_recall 0.7058196460644717\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "path_file='best_model_json/bestModel_birnnscrat.json'\n",
        "with open(path_file,mode='r') as f:\n",
        "    params = json.load(f)\n",
        "for key in params:\n",
        "    if params[key] == 'True':\n",
        "          params[key]=True\n",
        "    elif params[key] == 'False':\n",
        "          params[key]=False\n",
        "    if( key in ['batch_size','num_classes','hidden_size','supervised_layer_pos','num_supervised_heads','random_seed','max_length']):\n",
        "        if(params[key]!='N/A'):\n",
        "            params[key]=int(params[key])\n",
        "        \n",
        "    if((key == 'weights') and (params['auto_weights']==False)):\n",
        "        params[key] = ast.literal_eval(params[key])\n",
        "\n",
        "##### change in logging to output the results to neptune\n",
        "params['logging']='local'\n",
        "params['device']='cpu'\n",
        "params['best_params']=False\n",
        "\n",
        "if torch.cuda.is_available() and params['device']=='cuda':    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print('Since you dont want to use GPU, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "    \n",
        "    \n",
        "#### Few handy keys that you can directly change.\n",
        "params['variance']=1\n",
        "params['epochs']=1\n",
        "params['to_save']=True\n",
        "params['num_classes']=2\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "if(params['num_classes']==2 and (params['auto_weights']==False)):\n",
        "      params['weights']=[1.0,1.0]\n",
        "        \n",
        "#for att_lambda in [0.001,0.01,0.1,1,10,100]\n",
        "train_model(params,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORBj47ArF8-F",
        "outputId": "8474d200-b9dd-49a0-9307-99d665cef310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0796857 0.8201194 1.1703163]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [02:53,  2.76it/s]\n",
            "2it [00:00, 12.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.6838341473046\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:38, 12.60it/s]\n",
            "2it [00:00, 14.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.65\n",
            " Fscore: 0.63\n",
            " Precision: 0.66\n",
            " Recall: 0.62\n",
            " Roc Auc: 0.81\n",
            " Test took: 0:00:38\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:04, 14.57it/s]\n",
            "2it [00:00, 14.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.61\n",
            " Fscore: 0.59\n",
            " Precision: 0.63\n",
            " Recall: 0.59\n",
            " Roc Auc: 0.78\n",
            " Test took: 0:00:04\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:04, 14.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.62\n",
            " Fscore: 0.60\n",
            " Precision: 0.63\n",
            " Recall: 0.59\n",
            " Roc Auc: 0.78\n",
            " Test took: 0:00:04\n",
            "0.5908456676414833 0\n",
            "Saving model\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "best_val_fscore 0.5908456676414833\n",
            "best_test_fscore 0.5953800433596727\n",
            "best_val_rocauc 0.7813439473380873\n",
            "best_test_rocauc 0.7833791469223108\n",
            "best_val_precision 0.6323213563892517\n",
            "best_test_precision 0.6334081070577146\n",
            "best_val_recall 0.5854352674663518\n",
            "best_test_recall 0.5917058159561318\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "params['num_classes']=3\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "if(params['num_classes']==2 and (params['auto_weights']==False)):\n",
        "      params['weights']=[1.0,1.0]\n",
        "        \n",
        "#for att_lambda in [0.001,0.01,0.1,1,10,100]\n",
        "train_model(params,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRkvPokoMg3f",
        "outputId": "d9a87a8f-d2ba-4f07-be43-95340ffbec9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34jLhxdQ5stq",
        "outputId": "dfcbb523-6851-40bf-e997-b3e8d80fb8ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "Reading english - 1grams ...\n",
            "Since you dont want to use GPU, using the CPU instead.\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x5e6da000 @  0x7fb11e4191e7 0x4b2590 0x5ad01c 0x5e46ad 0x58f90f 0x59172f 0x591ac9 0x4fc06a 0x4fc808 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b575e 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fb11e016c87 0x5b636a\n",
            "tcmalloc: large alloc 2300993536 bytes == 0xe7940000 @  0x7fb11e4191e7 0x4b2590 0x5ad01c 0x4fc81a 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b575e 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fb11e016c87 0x5b636a\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=['hatespeech' 'normal' 'offensive'], y=['normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  FutureWarning)\n",
            "total_data 1142\n",
            "100% 1142/1142 [00:01<00:00, 633.95it/s]\n",
            "100% 1142/1142 [00:00<00:00, 9405.76it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "Running eval on test data...\n",
            "100% 36/36 [00:02<00:00, 15.08it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " Accuracy: 0.472\n",
            " Fscore: 0.386\n",
            " Precision: 0.508\n",
            " Recall: 0.312\n",
            " Test took: 0:00:02\n",
            "100% 1142/1142 [00:00<00:00, 3966.67it/s]\n",
            "Since you dont want to use GPU, using the CPU instead.\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x636dc000 @  0x7fb11e4191e7 0x4b2590 0x5ad01c 0x5e46ad 0x58f90f 0x59172f 0x591ac9 0x4fc06a 0x4fc808 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b575e 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fb11e016c87 0x5b636a\n",
            "tcmalloc: large alloc 2300993536 bytes == 0xec942000 @  0x7fb11e4191e7 0x4b2590 0x5ad01c 0x4fc81a 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b575e 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fb11e016c87 0x5b636a\n",
            "100% 1142/1142 [00:00<00:00, 6450.51it/s]\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "Running eval on test data...\n",
            "100% 36/36 [00:02<00:00, 15.38it/s]\n",
            "100% 1142/1142 [00:00<00:00, 1777.26it/s]\n",
            "Since you dont want to use GPU, using the CPU instead.\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x636dc000 @  0x7fb11e4191e7 0x4b2590 0x5ad01c 0x5e46ad 0x58f90f 0x59172f 0x591ac9 0x4fc06a 0x4fc808 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b575e 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fb11e016c87 0x5b636a\n",
            "tcmalloc: large alloc 2300993536 bytes == 0xec942000 @  0x7fb11e4191e7 0x4b2590 0x5ad01c 0x4fc81a 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b575e 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fb11e016c87 0x5b636a\n",
            "100% 1142/1142 [00:00<00:00, 6248.52it/s]\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "Running eval on test data...\n",
            "100% 36/36 [00:02<00:00, 14.64it/s]\n",
            "\u001b[0m/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "Reading english - 1grams ...\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x5eb20000 @  0x7fe32f2ce1e7 0x4b2590 0x5ad01c 0x5e46ad 0x58f90f 0x59172f 0x591ac9 0x4fc06a 0x4fc808 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fe32eecbc87 0x5b636a\n",
            "tcmalloc: large alloc 2300993536 bytes == 0xe7d86000 @  0x7fe32f2ce1e7 0x4b2590 0x5ad01c 0x4fc81a 0x4fe70d 0x5f0318 0x58f62c 0x5105e2 0x58fd37 0x50c4fc 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x6005a3 0x607796 0x60785c 0x60a436 0x64db82 0x64dd2e 0x7fe32eecbc87 0x5b636a\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=['non-toxic' 'toxic'], y=['non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  FutureWarning)\n",
            "total_data 1924\n",
            "100% 1924/1924 [00:03<00:00, 485.97it/s]\n",
            "100% 1924/1924 [00:00<00:00, 5382.35it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Saved/birnnscrat_lstm_64_2_100.pth\n",
            "Running eval on test data...\n",
            "100% 61/61 [00:06<00:00, 10.14it/s]\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python testing_with_rational.py birnn_scrat 100\n",
        "!python testing_for_bias.py birnn_scrat 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SynOxIW5PY-v",
        "outputId": "89b43d84-18a9-4888-ece6-030d24af0efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestModel_birnnscrat_100_explanation_top5.json\tbestModel_birnnscrat_bias.json\n"
          ]
        }
      ],
      "source": [
        "!ls explanations_dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1-wjKhvNrF5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Bias Calculation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GAPagtSDQ9zo"
      },
      "outputs": [],
      "source": [
        "from collections import Counter,defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lFOmyTVIRJ7R"
      },
      "outputs": [],
      "source": [
        "# get_annotated_data method is used to load the dataset\n",
        "from Preprocess.dataCollect import get_annotated_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkPyAXgDPqDh",
        "outputId": "47f3cce1-26d1-417b-a71d-9b491209bd2e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HateXplain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iw2qaVXLROW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8495f3bb-727c-4165-c8da-e4ad4de8441b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20197"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "dict_data_folder={\n",
        "      '2':{'data_file':'/content/HateXplain/Data/new2_dataset.json','class_label':'Data/classes_two.npy'},\n",
        "      '3':{'data_file':'/content/HateXplain/Data/new2_dataset.json','class_label':'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "params = {}\n",
        "\n",
        "# We need to load the dataset with the labels as 'toxic' and 'non-toxic'. \n",
        "# We consider hatespeech and offensive as toxic and normal as non-toxic.\n",
        "params['num_classes']=2  \n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "data_all_labelled=get_annotated_data(params)\n",
        "len(data_all_labelled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LDRrrN8CRRfg",
        "outputId": "07a3d489-749f-4bb1-a8b5-cff948a129d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           post_id  \\\n",
              "0      1179055004553900032_twitter   \n",
              "1      1179063826874032128_twitter   \n",
              "2      1178793830532956161_twitter   \n",
              "3      1179088797964763136_twitter   \n",
              "4      1179085312976445440_twitter   \n",
              "...                            ...   \n",
              "20192    20221289450717841_tweeter   \n",
              "20193   202222456621025979_tweeter   \n",
              "20194   202278595017479256_tweeter   \n",
              "20195   202215785677977454_tweeter   \n",
              "20196   202289270466794431_tweeter   \n",
              "\n",
              "                                                    text annotatorid1  \\\n",
              "0      [i, dont, think, im, getting, my, baby, them, ...            1   \n",
              "1      [we, cannot, continue, calling, ourselves, fem...            1   \n",
              "2                    [nawt, yall, niggers, ignoring, me]            4   \n",
              "3      [<user>, i, am, bit, confused, coz, chinese, p...            1   \n",
              "4      [this, bitch, in, whataburger, eating, a, burg...            4   \n",
              "...                                                  ...          ...   \n",
              "20192  [, Covid, was, released, from, a, lab, in, China]       Abreen   \n",
              "20193  [, LOOOOL, a, suit, from, a, big, Cubano, clea...       Abreen   \n",
              "20194  [, LOOOOL, a, suit, from, a, big, Cubano, clea...       Abreen   \n",
              "20195  [, It, was, happening, before, the, so, we, ca...       Abreen   \n",
              "20196                                [, Just, the, shot]       Abreen   \n",
              "\n",
              "                  target1       label1 annotatorid2             target2  \\\n",
              "0                  [None]       normal            2              [None]   \n",
              "1                  [None]       normal            2              [None]   \n",
              "2               [African]       normal            2              [None]   \n",
              "3                 [Asian]   hatespeech            4             [Asian]   \n",
              "4      [Caucasian, Women]   hatespeech            2  [Women, Caucasian]   \n",
              "...                   ...          ...          ...                 ...   \n",
              "20192             [Asian]  hatespeech         Clary             [Asian]   \n",
              "20193                  []       normal        Clary                  []   \n",
              "20194                  []       normal        Clary                  []   \n",
              "20195                  []       normal        Clary                  []   \n",
              "20196                  []       normal        Clary                  []   \n",
              "\n",
              "            label2 annotatorid3             target3       label3  \\\n",
              "0           normal            3              [None]       normal   \n",
              "1           normal            3              [None]       normal   \n",
              "2           normal            3           [African]   hatespeech   \n",
              "3        offensive            3             [Asian]   hatespeech   \n",
              "4       hatespeech            3  [Women, Caucasian]    offensive   \n",
              "...            ...          ...                 ...          ...   \n",
              "20192  hatespeech           Lei             [Asian]  hatespeech    \n",
              "20193       normal          Lei                  []       normal   \n",
              "20194       normal          Lei                  []       normal   \n",
              "20195       normal          Lei                  []       normal   \n",
              "20196       normal          Lei                  []       normal   \n",
              "\n",
              "                                              rationales final_label  \n",
              "0                                                     []   non-toxic  \n",
              "1                                                     []   non-toxic  \n",
              "2                                                     []   non-toxic  \n",
              "3      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       toxic  \n",
              "4      [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       toxic  \n",
              "...                                                  ...         ...  \n",
              "20192  [[0, 0, 1, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 1, ...   non-toxic  \n",
              "20193  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   non-toxic  \n",
              "20194  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   non-toxic  \n",
              "20195  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [...   non-toxic  \n",
              "20196                  [[0, 0, 0], [0, 0, 0], [0, 0, 0]]   non-toxic  \n",
              "\n",
              "[20197 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8bd91d67-1053-4c6f-ab50-339579e6b72f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>text</th>\n",
              "      <th>annotatorid1</th>\n",
              "      <th>target1</th>\n",
              "      <th>label1</th>\n",
              "      <th>annotatorid2</th>\n",
              "      <th>target2</th>\n",
              "      <th>label2</th>\n",
              "      <th>annotatorid3</th>\n",
              "      <th>target3</th>\n",
              "      <th>label3</th>\n",
              "      <th>rationales</th>\n",
              "      <th>final_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1179055004553900032_twitter</td>\n",
              "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>2</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>3</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1179063826874032128_twitter</td>\n",
              "      <td>[we, cannot, continue, calling, ourselves, fem...</td>\n",
              "      <td>1</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>2</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>3</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1178793830532956161_twitter</td>\n",
              "      <td>[nawt, yall, niggers, ignoring, me]</td>\n",
              "      <td>4</td>\n",
              "      <td>[African]</td>\n",
              "      <td>normal</td>\n",
              "      <td>2</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>3</td>\n",
              "      <td>[African]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1179088797964763136_twitter</td>\n",
              "      <td>[&lt;user&gt;, i, am, bit, confused, coz, chinese, p...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>4</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>3</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1179085312976445440_twitter</td>\n",
              "      <td>[this, bitch, in, whataburger, eating, a, burg...</td>\n",
              "      <td>4</td>\n",
              "      <td>[Caucasian, Women]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>2</td>\n",
              "      <td>[Women, Caucasian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>3</td>\n",
              "      <td>[Women, Caucasian]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20192</th>\n",
              "      <td>20221289450717841_tweeter</td>\n",
              "      <td>[, Covid, was, released, from, a, lab, in, China]</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 1, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 1, ...</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20193</th>\n",
              "      <td>202222456621025979_tweeter</td>\n",
              "      <td>[, LOOOOL, a, suit, from, a, big, Cubano, clea...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20194</th>\n",
              "      <td>202278595017479256_tweeter</td>\n",
              "      <td>[, LOOOOL, a, suit, from, a, big, Cubano, clea...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20195</th>\n",
              "      <td>202215785677977454_tweeter</td>\n",
              "      <td>[, It, was, happening, before, the, so, we, ca...</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [...</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20196</th>\n",
              "      <td>202289270466794431_tweeter</td>\n",
              "      <td>[, Just, the, shot]</td>\n",
              "      <td>Abreen</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>Clary</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>Lei</td>\n",
              "      <td>[]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[[0, 0, 0], [0, 0, 0], [0, 0, 0]]</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20197 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8bd91d67-1053-4c6f-ab50-339579e6b72f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8bd91d67-1053-4c6f-ab50-339579e6b72f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8bd91d67-1053-4c6f-ab50-339579e6b72f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "data_all_labelled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "14K9-nzFRU8B"
      },
      "outputs": [],
      "source": [
        "def generate_target_information(dataset):\n",
        "    final_target_output = defaultdict(list)\n",
        "    all_communities_selected = []\n",
        "    \n",
        "    for each in dataset.iterrows(): \n",
        "        # All the target communities tagged for this post\n",
        "        all_targets = each[1]['target1']+each[1]['target2']+each[1]['target3']  \n",
        "        community_dict = dict(Counter(all_targets))\n",
        "        \n",
        "        # Select only those communities which are present more than once.\n",
        "        for key in community_dict:\n",
        "            if community_dict[key]>1:  \n",
        "                final_target_output[each[1]['post_id']].append(key)\n",
        "                all_communities_selected.append(key)\n",
        "        \n",
        "        # If no community is selected based on majority voting then we don't select any community\n",
        "        if each[1]['post_id'] not in final_target_output:\n",
        "            final_target_output[each[1]['post_id']].append('None')\n",
        "            all_communities_selected.append(key)\n",
        "\n",
        "    return final_target_output, all_communities_selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mEmm7AD9Ra13"
      },
      "outputs": [],
      "source": [
        "target_information, all_communities_selected = generate_target_information(data_all_labelled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPCh_pj3ReFA",
        "outputId": "77d83713-e09f-4e49-a363-b19ec5dd8558"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['African',\n",
              " 'Islam',\n",
              " 'Jewish',\n",
              " 'Homosexual',\n",
              " 'Women',\n",
              " 'Refugee',\n",
              " 'Arab',\n",
              " 'Caucasian',\n",
              " 'Asian',\n",
              " 'Hispanic']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "community_count_dict = Counter(all_communities_selected)\n",
        "\n",
        "# We remove None and Other from dictionary\n",
        "community_count_dict.pop('None')\n",
        "community_count_dict.pop('Other')\n",
        "\n",
        "# For the bias calculation, we are considering the top 10 communites based on their count\n",
        "list_selected_community = [community for community, value in community_count_dict.most_common(10)]\n",
        "list_selected_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "eRU3peSrRhVo"
      },
      "outputs": [],
      "source": [
        "# Based on the top 10 communities, we filter the target_information\n",
        "# This will remove the other communities from the calculation\n",
        "\n",
        "final_target_information ={}\n",
        "for each in target_information:\n",
        "    temp = list(set(target_information[each])&set(list_selected_community))\n",
        "    if len(temp) == 0:\n",
        "        final_target_information[each] = None\n",
        "    else:\n",
        "        final_target_information[each] = temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BEsvl0xaRkpw"
      },
      "outputs": [],
      "source": [
        "# Add a new column 'final_target_category' which will contain the selected target community names\n",
        "data_all_labelled['final_target_category'] = data_all_labelled['post_id'].map(final_target_information)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L0-Y6iMiRnxH",
        "outputId": "4cb1bbd2-0231-450d-dfc7-1fc7dbc480a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           post_id  \\\n",
              "0      1179055004553900032_twitter   \n",
              "5                     13851720_gab   \n",
              "17     1178818409812746240_twitter   \n",
              "25                    19346774_gab   \n",
              "36     1165819983701643266_twitter   \n",
              "...                            ...   \n",
              "20105                  9773208_gab   \n",
              "20109                  9802943_gab   \n",
              "20117                  9826963_gab   \n",
              "20128                  9867117_gab   \n",
              "20130                  9869181_gab   \n",
              "\n",
              "                                                    text annotatorid1  \\\n",
              "0      [i, dont, think, im, getting, my, baby, them, ...            1   \n",
              "5      [laura, loomer, raped, me, while, screaming, a...            1   \n",
              "17     [<user>, what, did, the, old, lady, do, was, s...            9   \n",
              "25     [as, much, as, i, appreciate, a, format, of, o...            9   \n",
              "36     [sex, be, so, good, a, bitch, be, slow, stroki...            4   \n",
              "...                                                  ...          ...   \n",
              "20105  [it, always, women, trying, this, shit, like, ...          200   \n",
              "20109  [because, women, would, never, lie, about, bei...          228   \n",
              "20117  [how, is, all, that, awesome, muslim, diversit...          228   \n",
              "20128  [well, my, dear, lgbtq, brothers, and, sisters...          207   \n",
              "20130  [but, it, on, hbo, more, violence, and, sex, c...          223   \n",
              "\n",
              "              target1      label1 annotatorid2   target2      label2  \\\n",
              "0              [None]      normal            2    [None]      normal   \n",
              "5            [Jewish]  hatespeech            2  [Jewish]  hatespeech   \n",
              "17             [None]      normal           10    [None]      normal   \n",
              "25             [None]      normal           13    [None]      normal   \n",
              "36            [Women]   offensive            7   [Women]   offensive   \n",
              "...               ...         ...          ...       ...         ...   \n",
              "20105         [Women]  hatespeech          202   [Women]   offensive   \n",
              "20109         [Women]   offensive          222   [Women]   offensive   \n",
              "20117         [Islam]   offensive          222   [Islam]   offensive   \n",
              "20128  [Islam, Other]   offensive          223   [Islam]  hatespeech   \n",
              "20130          [None]      normal          200    [None]      normal   \n",
              "\n",
              "      annotatorid3     target3      label3  \\\n",
              "0                3      [None]      normal   \n",
              "5                3    [Jewish]  hatespeech   \n",
              "17               4      [None]      normal   \n",
              "25               4  [Hispanic]   offensive   \n",
              "36              16      [None]      normal   \n",
              "...            ...         ...         ...   \n",
              "20105          203     [Women]   offensive   \n",
              "20109          209     [Women]      normal   \n",
              "20117          209     [Islam]   offensive   \n",
              "20128          231     [Islam]  hatespeech   \n",
              "20130          233      [None]      normal   \n",
              "\n",
              "                                              rationales final_label  \\\n",
              "0                                                     []   non-toxic   \n",
              "5      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,...       toxic   \n",
              "17                                                    []   non-toxic   \n",
              "25                                                    []   non-toxic   \n",
              "36     [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, ...       toxic   \n",
              "...                                                  ...         ...   \n",
              "20105  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...       toxic   \n",
              "20109  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...       toxic   \n",
              "20117  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...       toxic   \n",
              "20128  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       toxic   \n",
              "20130                                                 []   non-toxic   \n",
              "\n",
              "      final_target_category  \n",
              "0                      None  \n",
              "5                  [Jewish]  \n",
              "17                     None  \n",
              "25                     None  \n",
              "36                  [Women]  \n",
              "...                     ...  \n",
              "20105               [Women]  \n",
              "20109               [Women]  \n",
              "20117               [Islam]  \n",
              "20128               [Islam]  \n",
              "20130                  None  \n",
              "\n",
              "[1924 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc476b52-e125-41b0-a3d3-903a933fc793\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>text</th>\n",
              "      <th>annotatorid1</th>\n",
              "      <th>target1</th>\n",
              "      <th>label1</th>\n",
              "      <th>annotatorid2</th>\n",
              "      <th>target2</th>\n",
              "      <th>label2</th>\n",
              "      <th>annotatorid3</th>\n",
              "      <th>target3</th>\n",
              "      <th>label3</th>\n",
              "      <th>rationales</th>\n",
              "      <th>final_label</th>\n",
              "      <th>final_target_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1179055004553900032_twitter</td>\n",
              "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>2</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>3</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>13851720_gab</td>\n",
              "      <td>[laura, loomer, raped, me, while, screaming, a...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Jewish]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>2</td>\n",
              "      <td>[Jewish]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>3</td>\n",
              "      <td>[Jewish]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Jewish]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1178818409812746240_twitter</td>\n",
              "      <td>[&lt;user&gt;, what, did, the, old, lady, do, was, s...</td>\n",
              "      <td>9</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>10</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>4</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>19346774_gab</td>\n",
              "      <td>[as, much, as, i, appreciate, a, format, of, o...</td>\n",
              "      <td>9</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>13</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>4</td>\n",
              "      <td>[Hispanic]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1165819983701643266_twitter</td>\n",
              "      <td>[sex, be, so, good, a, bitch, be, slow, stroki...</td>\n",
              "      <td>4</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>7</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>16</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, ...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Women]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20105</th>\n",
              "      <td>9773208_gab</td>\n",
              "      <td>[it, always, women, trying, this, shit, like, ...</td>\n",
              "      <td>200</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>202</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>203</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Women]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20109</th>\n",
              "      <td>9802943_gab</td>\n",
              "      <td>[because, women, would, never, lie, about, bei...</td>\n",
              "      <td>228</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>222</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>209</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Women]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20117</th>\n",
              "      <td>9826963_gab</td>\n",
              "      <td>[how, is, all, that, awesome, muslim, diversit...</td>\n",
              "      <td>228</td>\n",
              "      <td>[Islam]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>222</td>\n",
              "      <td>[Islam]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>209</td>\n",
              "      <td>[Islam]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Islam]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20128</th>\n",
              "      <td>9867117_gab</td>\n",
              "      <td>[well, my, dear, lgbtq, brothers, and, sisters...</td>\n",
              "      <td>207</td>\n",
              "      <td>[Islam, Other]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>223</td>\n",
              "      <td>[Islam]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>231</td>\n",
              "      <td>[Islam]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Islam]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20130</th>\n",
              "      <td>9869181_gab</td>\n",
              "      <td>[but, it, on, hbo, more, violence, and, sex, c...</td>\n",
              "      <td>223</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>200</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>233</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1924 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc476b52-e125-41b0-a3d3-903a933fc793')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc476b52-e125-41b0-a3d3-903a933fc793 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc476b52-e125-41b0-a3d3-903a933fc793');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.\n",
        "postpost_id_divisions_path = '/content/HateXplain/Data/final2_post_id_divisions.json'\n",
        "\n",
        "with open(postpost_id_divisions_path, 'r') as fp:\n",
        "    post_id_dict=json.load(fp)\n",
        "\n",
        "data_all_labelled_bias = data_all_labelled[data_all_labelled['post_id'].isin(post_id_dict['test'])]\n",
        "data_all_labelled_bias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_all_labelled_bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLj9nT9WGKBZ",
        "outputId": "420d5df7-9c29-4f9e-aace-aff0640aeb2a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1924"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "uT3uzWgcRqaf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# The names of the files which will contain the bias output for each model\n",
        "bias_score_file_mapping={\n",
        "    'BiRNN-Attn':'bestModel_birnnscrat_bias.json',\n",
        "}\n",
        "\n",
        "# The parent folder path of the bias output files\n",
        "parent_path = './explanations_dicts/'\n",
        "\n",
        "# The bias methods that will be considered\n",
        "method_list = ['subgroup', 'bpsn', 'bnsp']\n",
        "\n",
        "community_list = list(list_selected_community)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "X0H9nMZNR1v3"
      },
      "outputs": [],
      "source": [
        "# This function is used to convert the classification into a [0-1] score \n",
        "# with a value of 0 meaning non-toxic and 1 meaning toxic\n",
        "def convert_to_score(label_name, label_dict):\n",
        "    if label_name=='non-toxic':\n",
        "        return 1-label_dict[label_name]\n",
        "    else:\n",
        "        return label_dict[label_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qUFyY8iVR4kn"
      },
      "outputs": [],
      "source": [
        "# Function to divide the ids into postive or class class based on the method.\n",
        "def bias_evaluation_metric(dataset, method, community):\n",
        "    positive_ids = []\n",
        "    negative_ids = []\n",
        "    if method=='subgroup':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "            else:\n",
        "                pass\n",
        "    elif method=='bpsn':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "#                 print(eachrow[1]['final_label'])\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "            else:\n",
        "                if eachrow[1]['final_label'] !='non-toxic':\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "    elif method=='bnsp':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "                if eachrow[1]['final_label'] !='non-toxic':\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "            else:\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "    else:\n",
        "        print('Incorrect option selected!!!')\n",
        "                \n",
        "    return {'positiveID':positive_ids, 'negativeID':negative_ids}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "022bfdd303844a71869661472bed2a7b",
            "47a0dee2b1164c679be3ba3c124abb18",
            "839dee59022d4a03a5205e62718f2903",
            "fa0658725b284c5eba40b7e91faee507",
            "0a2d680fd6bb4b05acabf1691af147d2",
            "856c96f5e6324e488d8129be7f930ed3",
            "6cc81c22d72143f7a35a23f81fd025cf",
            "e3c350c0b5f2431687b4ca753b1b1423"
          ]
        },
        "id": "_o-CBxRFR7YQ",
        "outputId": "6b88d7a4-9bd3-43ca-d1ba-0bfdc62938dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "022bfdd303844a71869661472bed2a7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "final_bias_dictionary = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "# We load each of the model bias output file and compute the bias score using each method for all the community\n",
        "for each_model in tqdm(bias_score_file_mapping):\n",
        "    total_data ={}\n",
        "    with open(parent_path+bias_score_file_mapping[each_model]) as fp:\n",
        "        for line in fp:\n",
        "            data = json.loads(line)\n",
        "            total_data[data['annotation_id']] = data\n",
        "    for each_method in method_list:\n",
        "        for each_community in community_list:\n",
        "            community_data = bias_evaluation_metric(data_all_labelled_bias, each_method, each_community)\n",
        "            truth_values = []\n",
        "            prediction_values = []\n",
        "\n",
        "\n",
        "            label_to_value = {'toxic':1.0, 'non-toxic':0.0}\n",
        "            for each in community_data['positiveID']:\n",
        "                truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
        "                prediction_values.append(convert_to_score(total_data[each]['classification'], total_data[each]['classification_scores']))\n",
        "\n",
        "            for each in community_data['negativeID']:\n",
        "                truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
        "                prediction_values.append(convert_to_score(total_data[each]['classification'], total_data[each]['classification_scores']))\n",
        "\n",
        "            roc_output_value = roc_auc_score(truth_values, prediction_values)\n",
        "            final_bias_dictionary[each_model][each_method][each_community] = roc_output_value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(total_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUcLIMyhBzwT",
        "outputId": "51edb63c-b4d6-4a99-bbb0-dabb41ff0190"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1924"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_uoz4r8JR-23",
        "outputId": "26dd4edc-7e71-4d22-ddd4-ce52314b5b4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'%.4f'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "%precision 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PIQi-Am9SCIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df181499-465a-4dd0-adf4-86a17ff2a25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiRNN-Attn subgroup 0.6754469363110213\n",
            "BiRNN-Attn bpsn 0.6101327177142991\n",
            "BiRNN-Attn bnsp 0.6447702695441282\n"
          ]
        }
      ],
      "source": [
        "# To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:\n",
        "power_value = -5\n",
        "num_communities = len(community_list)\n",
        "\n",
        "for each_model in final_bias_dictionary:\n",
        "    for each_method in final_bias_dictionary[each_model]:\n",
        "        temp_value =[]\n",
        "        for each_community in final_bias_dictionary[each_model][each_method]:\n",
        "            temp_value.append(pow(final_bias_dictionary[each_model][each_method][each_community], power_value))\n",
        "        print(each_model, each_method, pow(np.sum(temp_value)/num_communities, 1/power_value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cos2FyRyScI6"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Explainability Calculation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "L0n04ccES0G3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import more_itertools as mit\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "DQ37OLVZS8gB"
      },
      "outputs": [],
      "source": [
        "# get_annotated_data method is used to load the dataset\n",
        "from Preprocess import *\n",
        "from Preprocess.dataCollect import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "8AZCJG3wS-ko"
      },
      "outputs": [],
      "source": [
        "dict_data_folder={\n",
        "      '2':{'data_file':'/content/HateXplain/Data/new2_dataset.json','class_label':'Data/classes_two.npy'},\n",
        "      '3':{'data_file':'/content/HateXplain/Data/new2_dataset.json','class_label':'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "# We need to load the dataset with the labels as 'hatespeech', 'offensive', and 'normal' (3-class). \n",
        "\n",
        "params = {}\n",
        "params['num_classes']=3\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "data_all_labelled=get_annotated_data(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5DLppxPTAjo",
        "outputId": "d49f84e4-d8dd-49fd-afe1-f6649b8cd108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Normal tokenizer...\n"
          ]
        }
      ],
      "source": [
        "# The important key here is the 'bert_token'. Set it to True for Bert based models and False for Others.\n",
        "\n",
        "params_data={\n",
        "    'include_special':False,  #True is want to include <url> in place of urls if False will be removed\n",
        "    'bert_tokens':False, #True /False\n",
        "    'type_attention':'softmax', #softmax\n",
        "    'set_decay':0.1,\n",
        "    'majority':2,\n",
        "    'max_length':128,\n",
        "    'variance':10,\n",
        "    'window':4,\n",
        "    'alpha':0.5,\n",
        "    'p_value':0.8,\n",
        "    'method':'additive',\n",
        "    'decay':False,\n",
        "    'normalized':False,\n",
        "    'not_recollect':True,\n",
        "}\n",
        "\n",
        "\n",
        "if(params_data['bert_tokens']):\n",
        "    print('Loading BERT tokenizer...')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
        "else:\n",
        "    print('Loading Normal tokenizer...')\n",
        "    tokenizer=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Aqwkmy9ITEvH"
      },
      "outputs": [],
      "source": [
        "# Load the whole dataset and get the tokenwise rationales\n",
        "def get_training_data(data):\n",
        "    post_ids_list=[]\n",
        "    text_list=[]\n",
        "    attention_list=[]\n",
        "    label_list=[]\n",
        "    \n",
        "    final_binny_output = []\n",
        "    print('total_data',len(data))\n",
        "    for index,row in tqdm(data.iterrows(),total=len(data)):\n",
        "        annotation=row['final_label']\n",
        "        \n",
        "        text=row['text']\n",
        "        post_id=row['post_id']\n",
        "        annotation_list=[row['label1'],row['label2'],row['label3']]\n",
        "        tokens_all = list(row['text'])\n",
        "#         attention_masks =  [list(row['explain1']),list(row['explain2']),list(row['explain1'])]\n",
        "        \n",
        "        if(annotation!= 'undecided'):\n",
        "            tokens_all,attention_masks=returnMask(row, params_data, tokenizer)\n",
        "            final_binny_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])\n",
        "\n",
        "    return final_binny_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2lkIo1ATHjH",
        "outputId": "5490ac90-3eb6-4d22-fafc-9b50b8bcb7ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 63/20197 [00:00<00:32, 625.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_data 20197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20197/20197 [00:29<00:00, 674.02it/s]\n"
          ]
        }
      ],
      "source": [
        "training_data=get_training_data(data_all_labelled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "QZxfQUvdTJzn"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list\n",
        "def find_ranges(iterable):\n",
        "    \"\"\"Yield range of consecutive numbers.\"\"\"\n",
        "    for group in mit.consecutive_groups(iterable):\n",
        "        group = list(group)\n",
        "        if len(group) == 1:\n",
        "            yield group[0]\n",
        "        else:\n",
        "            yield group[0], group[-1]\n",
        "            \n",
        "# Convert dataset into ERASER format: https://github.com/jayded/eraserbenchmark/blob/master/rationale_benchmark/utils.py\n",
        "def get_evidence(post_id, anno_text, explanations):\n",
        "    output = []\n",
        "\n",
        "    indexes = sorted([i for i, each in enumerate(explanations) if each==1])\n",
        "    span_list = list(find_ranges(indexes))\n",
        "\n",
        "    for each in span_list:\n",
        "        if type(each)== int:\n",
        "            start = each\n",
        "            end = each+1\n",
        "        elif len(each) == 2:\n",
        "            start = each[0]\n",
        "            end = each[1]+1\n",
        "        else:\n",
        "            print('error')\n",
        "\n",
        "        output.append({\"docid\":post_id, \n",
        "              \"end_sentence\": -1, \n",
        "              \"end_token\": end, \n",
        "              \"start_sentence\": -1, \n",
        "              \"start_token\": start, \n",
        "              \"text\": ' '.join([str(x) for x in anno_text[start:end]])})\n",
        "    return output\n",
        "\n",
        "# To use the metrices defined in ERASER, we will have to convert the dataset\n",
        "def convert_to_eraser_format(dataset, method, save_split, save_path, id_division):  \n",
        "    final_output = []\n",
        "    \n",
        "    if save_split:\n",
        "        train_fp = open(save_path+'train.jsonl', 'w')\n",
        "        val_fp = open(save_path+'val.jsonl', 'w')\n",
        "        test_fp = open(save_path+'test.jsonl', 'w')\n",
        "            \n",
        "    for tcount, eachrow in enumerate(dataset):\n",
        "        \n",
        "        temp = {}\n",
        "        post_id = eachrow[0]\n",
        "        post_class = eachrow[1]\n",
        "        anno_text_list = eachrow[2]\n",
        "        majority_label = eachrow[1]\n",
        "        \n",
        "        if majority_label=='normal':\n",
        "            continue\n",
        "        \n",
        "        all_labels = eachrow[4]\n",
        "        explanations = []\n",
        "        for each_explain in eachrow[3]:\n",
        "            explanations.append(list(each_explain))\n",
        "        \n",
        "        # For this work, we have considered the union of explanations. Other options could be explored as well.\n",
        "        if method == 'union':\n",
        "            final_explanation = [any(each) for each in zip(*explanations)]\n",
        "            final_explanation = [int(each) for each in final_explanation]\n",
        "        \n",
        "            \n",
        "        temp['annotation_id'] = post_id\n",
        "        temp['classification'] = post_class\n",
        "        temp['evidences'] = [get_evidence(post_id, list(anno_text_list), final_explanation)]\n",
        "        temp['query'] = \"What is the class?\"\n",
        "        temp['query_type'] = None\n",
        "        final_output.append(temp)\n",
        "        \n",
        "        if save_split:\n",
        "            if not os.path.exists(save_path+'docs'):\n",
        "                os.makedirs(save_path+'docs')\n",
        "            \n",
        "            with open(save_path+'docs/'+post_id, 'w') as fp:\n",
        "                fp.write(' '.join([str(x) for x in list(anno_text_list)]))\n",
        "            \n",
        "            if post_id in id_division['train']:\n",
        "                train_fp.write(json.dumps(temp)+'\\n')\n",
        "            \n",
        "            elif post_id in id_division['val']:\n",
        "                val_fp.write(json.dumps(temp)+'\\n')\n",
        "            \n",
        "            elif post_id in id_division['test']:\n",
        "                test_fp.write(json.dumps(temp)+'\\n')\n",
        "            else:\n",
        "                print(post_id)\n",
        "    \n",
        "    if save_split:\n",
        "        train_fp.close()\n",
        "        val_fp.close()\n",
        "        test_fp.close()\n",
        "        \n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "iDjuBSBvqoyt",
        "outputId": "b8ee0b5b-b977-406c-eea3-b9457f4b75e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HateXplain/eraserbenchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "deYKnU3wTRJn"
      },
      "outputs": [],
      "source": [
        "# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.\n",
        "with open('/content/HateXplain/Data/final2_post_id_divisions.json') as fp:\n",
        "    id_division = json.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "XlA0iMeETjUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4773e904-4eef-4190-adf3-ebc84cc0e484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/HateXplain/Data/Evaluation’: File exists\n",
            "mkdir: cannot create directory ‘/content/HateXplain/Data/Evaluation/Model_Eval’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir /content/HateXplain/Data/Evaluation\n",
        "!mkdir /content/HateXplain/Data/Evaluation/Model_Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "B2XwjEPOTUaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6608001e-f87e-4846-f34e-73de1d00d101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "202271234819374164_tweeter\n",
            "202286621642816856_tweeter\n",
            "202270076496768511_tweeter\n",
            "20226174138634896_tweeter\n",
            "202294029739425649_tweeter\n",
            "202275083095983040_tweeter\n",
            "202251621052154917_tweeter\n",
            "202272686717140011_tweeter\n",
            "202221304463855513_tweeter\n",
            "202283547509701064_tweeter\n",
            "20225474354819644_tweeter\n",
            "202224057072620967_tweeter\n",
            "202235978087743292_tweeter\n",
            "20226200177305469_tweeter\n",
            "202271571706629254_tweeter\n",
            "2022441876275850_tweeter\n",
            "202274661837388198_tweeter\n",
            "202224252943863908_tweeter\n",
            "202220775735822625_tweeter\n",
            "202283501818322860_tweeter\n",
            "20224665237916916_tweeter\n",
            "202297443767427150_tweeter\n",
            "202221843521932442_tweeter\n",
            "202231447354105357_tweeter\n",
            "2022365432069241_tweeter\n",
            "202233250312176522_tweeter\n",
            "202244356376587534_tweeter\n",
            "202232399203405907_tweeter\n",
            "202255477615964561_tweeter\n",
            "202292508885698536_tweeter\n",
            "202269047287005382_tweeter\n",
            "20221289450717841_tweeter\n"
          ]
        }
      ],
      "source": [
        "method = 'union'\n",
        "save_split = True\n",
        "save_path = '/content/HateXplain/Data/Evaluation/Model_Eval/'  #The dataset in Eraser Format will be stored here.\n",
        "output_eraser = convert_to_eraser_format(training_data, method, save_split, save_path, id_division)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "e76FcTICTXrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d923384-03af-4b32-f1d9-4549cae5d6be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'Data/Evaluation/Model_Eval/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls Data/Evaluation/Model_Eval/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67kj7CSHBEWv",
        "outputId": "7afbbb10-3e0a-4722-afd3-41cf89417d7e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_model_json\t\t\t     Models\n",
            "best_runs.sh\t\t\t     Parameters_description.md\n",
            "Bias_Calculation_NB.ipynb\t     parameters_selection.py\n",
            "convert_to_word2vec.py\t\t     Preprocess\n",
            "Data\t\t\t\t     __pycache__\n",
            "eraserbenchmark\t\t\t     README.md\n",
            "Example_HateExplain.ipynb\t     requirements.txt\n",
            "Explainability_Calculation_NB.ipynb  Saved\n",
            "explanations_dicts\t\t     TensorDataset\n",
            "Figures\t\t\t\t     testing_for_bias.py\n",
            "HateXplain\t\t\t     testing_with_lime.py\n",
            "LICENSE\t\t\t\t     testing_with_rational.py\n",
            "manual_training_inference.py\t     test_parallel.sh\n",
            "model_explain_output.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd eraserbenchmark/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR973lE2BEaK",
        "outputId": "bf32558a-fbe2-499b-8853-cd4f2deb26d3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HateXplain/eraserbenchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z3BqZLFBEi_",
        "outputId": "c54c9a79-e152-454d-f326-4c18a55beb5e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_exploration.ipynb\tLICENSE  rationale_benchmark  REPRODUCTION.txt\n",
            "HateXplain\t\tparams\t README.md\t      requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "GzZLhJf-U8Vf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1bad64f-5119-4d97-bed4-39ba7a968ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1810 MainThread Error in instances: 0 instances fail validation: set()\n",
            "  3351 MainThread No sentence level predictions detected, skipping sentence-level diagnostic\n",
            "Traceback (most recent call last):\n",
            "  File \"rationale_benchmark/metrics.py\", line 670, in <module>\n",
            "    main()\n",
            "  File \"rationale_benchmark/metrics.py\", line 658, in main\n",
            "    class_results = score_classifications(results, annotations, flattened_documents, args.aopc_thresholds)\n",
            "  File \"rationale_benchmark/metrics.py\", line 293, in score_classifications\n",
            "    inst = key_to_instances[ann.annotation_id]\n",
            "KeyError: '202271558011728624_tweeter'\n"
          ]
        }
      ],
      "source": [
        "!PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py --split test  --data_dir ../Data/Evaluation/Model_Eval --results ../explanations_dicts/bestModel_birnnscrat_100_explanation_top5.json --score_file ../model_explain_output.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V59mqw02AlUP",
        "outputId": "c8ad355f-8c19-4175-c370-393da3955abd"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HateXplain/eraserbenchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "D1eQENR4VLp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7805544f-3515-4df4-a785-5f16de789dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Plausibility\n",
            "IOU F1 : 0.22556572791882537\n",
            "Token F1 : 0.4933001129231857\n",
            "AUPRC : 0.8179723648640134\n",
            "\n",
            "Faithfulness\n",
            "Comprehensiveness : 0.12690107161619676\n",
            "Sufficiency -0.008355937659035664\n"
          ]
        }
      ],
      "source": [
        "# print the required results\n",
        "with open('../model_explain_output.json') as fp:\n",
        "    output_data = json.load(fp)\n",
        "\n",
        "print('\\nPlausibility')\n",
        "print('IOU F1 :', output_data['iou_scores'][0]['macro']['f1'])\n",
        "print('Token F1 :', output_data['token_prf']['instance_macro']['f1'])\n",
        "print('AUPRC :', output_data['token_soft_metrics']['auprc'])\n",
        "\n",
        "print('\\nFaithfulness')\n",
        "print('Comprehensiveness :', output_data['classification_scores']['comprehensiveness'])\n",
        "print('Sufficiency', output_data['classification_scores']['sufficiency'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7DtYg-yNAkET"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "P-JVJuctN4EJ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuple_community = []\n",
        "for each_model in final_bias_dictionary:\n",
        "    # Select the metric which you want to view. Possible values are subgroup, bpsn, bnsp\n",
        "    each_method = 'bnsp' \n",
        "    \n",
        "    for each_community in final_bias_dictionary[each_model][each_method]:\n",
        "        tuple_community.append((each_model, each_community, final_bias_dictionary[each_model][each_method][each_community]))"
      ],
      "metadata": {
        "id": "MOB018lKN8oS"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_community_score = pd.DataFrame(tuple_community, columns=['Model', 'Community', 'AUCROC'])"
      ],
      "metadata": {
        "id": "FiJPk4f2N9J9"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.catplot(x=\"Community\", y=\"AUCROC\", hue=\"Model\",\n",
        "             data=df_community_score,\n",
        "                 legend=False,\n",
        "                kind=\"bar\");\n",
        "ax.set(ylim=(0.3, 1.0))\n",
        "ax.set_xticklabels(rotation=45, size=13, horizontalalignment='right')\n",
        "\n",
        "# sns.set(font_scale = 0.1)\n",
        "\n",
        "\n",
        "\n",
        "handles = ax._legend_data.values()\n",
        "labels = ax._legend_data.keys()\n",
        "\n",
        "ax.fig.legend(handles=handles, labels=labels, loc='upper right', ncol=3)\n",
        "ax.fig.subplots_adjust(top=0.92)\n",
        "\n",
        "ax.set(xlabel=\"\")  \n",
        "\n",
        "print(each_method)\n",
        "plt.savefig('bias-'+each_method+'.pdf', dpi=300, transparent=True, bbox_inches='tight')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "OZf78gzsN_OR",
        "outputId": "207a26d9-736f-45ed-d731-df2e1aeb4011"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bnsp\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "022bfdd303844a71869661472bed2a7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47a0dee2b1164c679be3ba3c124abb18",
              "IPY_MODEL_839dee59022d4a03a5205e62718f2903"
            ],
            "layout": "IPY_MODEL_fa0658725b284c5eba40b7e91faee507"
          }
        },
        "47a0dee2b1164c679be3ba3c124abb18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a2d680fd6bb4b05acabf1691af147d2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_856c96f5e6324e488d8129be7f930ed3",
            "value": 1
          }
        },
        "839dee59022d4a03a5205e62718f2903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cc81c22d72143f7a35a23f81fd025cf",
            "placeholder": "​",
            "style": "IPY_MODEL_e3c350c0b5f2431687b4ca753b1b1423",
            "value": " 1/1 [00:05&lt;00:00,  5.93s/it]"
          }
        },
        "fa0658725b284c5eba40b7e91faee507": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a2d680fd6bb4b05acabf1691af147d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "856c96f5e6324e488d8129be7f930ed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "6cc81c22d72143f7a35a23f81fd025cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3c350c0b5f2431687b4ca753b1b1423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}